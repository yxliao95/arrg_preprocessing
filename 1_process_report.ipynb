{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret-cxr reports\n",
    "\n",
    "Take interpret-cxr (hg datasets) as input.\n",
    "\n",
    "The output will be used as the input for arrg_sentgen\n",
    "\n",
    "Example output:\n",
    "\n",
    "```\n",
    "{\"doc_key\": \"train#0#impression\", \n",
    " \"sent_toks\": [[\"1.DECREASED\", \"BIBASILAR\", \"PARENCHYMAL\", \"OPACITIES\", \",\", \"NOW\", \"MINIMAL\", \".\"], [\"STABLE\", \"SMALL\", \"LEFT\", \"PLEURAL\", \"EFFUSION\", \".\"], [\"2\", \".\", \"FEEDING\", \"TUBE\", \"AND\", \"STERNAL\", \"PLATES\", \"AGAIN\", \"SEEN\", \".\"]], \n",
    " \"tok_char_indices\": [[[0, 11], [12, 21], [22, 33], [34, 43], [43, 44], [45, 48], [49, 56], [56, 57]], [[58, 64], [65, 70], [71, 75], [76, 83], [84, 92], [92, 93]], [[94, 95], [95, 96], [97, 104], [105, 109], [110, 113], [114, 121], [122, 128], [129, 134], [135, 139], [139, 140]]], \n",
    " \"sents\": [\"1.DECREASED BIBASILAR PARENCHYMAL OPACITIES, NOW MINIMAL.\", \"STABLE SMALL LEFT PLEURAL EFFUSION.\", \"2. FEEDING TUBE AND STERNAL PLATES AGAIN SEEN.\"], \n",
    " \"sent_char_indices\": [[0, 57], [58, 93], [94, 140]]}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/anaconda3/envs/arrg_proprocessing/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Sequence, Image, DatasetDict, concatenate_datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/yuxiang/liao/mimic/mimic-cxr-jpg/2.1.0\"\n",
    "PATH_PREFIX = \"/home/yuxiang/liao/mimic/mimic-cxr-jpg/2.1.0\"\n",
    "\n",
    "dataset_interpret = datasets.load_from_disk(\"/home/yuxiang/liao/resources/datasets/interpret-cxr\")\n",
    "dataset_interpret_test_public = datasets.load_from_disk(\"/home/yuxiang/liao/resources/datasets/interpret-cxr-test-public\")\n",
    "dataset_mimic = load_dataset(\"json\", data_files={\"train\": os.path.join(data_dir, \"train_mimic.json\"), \"validation\": os.path.join(data_dir, \"val_mimic.json\")})\n",
    "\n",
    "\n",
    "def add_prefix(example):\n",
    "    example[\"images\"] = [os.path.join(PATH_PREFIX, i) for i in example[\"images\"]]\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset_mimic = dataset_mimic.map(add_prefix, num_proc=8).cast_column(\"images\", Sequence(Image()))\n",
    "dataset_final = DatasetDict({\"train\": concatenate_datasets([dataset_interpret[\"train\"], dataset_mimic[\"train\"]]), \"validation\": concatenate_datasets([dataset_interpret[\"validation\"], dataset_mimic[\"validation\"]]), \"test\": dataset_interpret_test_public[\"test\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need this column at the moment. It will slowdown the iter speed\n",
    "dataset_final = dataset_final.remove_columns([\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'images_path', 'impression', 'findings'],\n",
       "        num_rows: 550395\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'images_path', 'impression', 'findings'],\n",
       "        num_rows: 14111\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['findings', 'impression'],\n",
       "        num_rows: 3677\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate raw_reports.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'parser']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"tagger\", \"attribute_ruler\", \"lemmatizer\", \"ner\"])\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550395/550395 [00:13<00:00, 39426.53it/s]\n",
      "100%|██████████| 14111/14111 [00:00<00:00, 41530.18it/s]\n",
      "100%|██████████| 3677/3677 [00:00<00:00, 71681.01it/s]\n"
     ]
    }
   ],
   "source": [
    "text_tuples = []\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    for idx, data in enumerate(tqdm(dataset_final[split])):\n",
    "        doc_key_prefix = f\"{split}#{idx}\"\n",
    "        if split == \"test\":\n",
    "            valid_key = \"\"\n",
    "        else:\n",
    "            valid_key = f'{data[\"source\"]}#{data[\"images_path\"][0]}'\n",
    "\n",
    "        text_tuples.append((data[\"findings\"], {\"doc_key\": f\"{doc_key_prefix}#findings\", \"valid_key\": valid_key}))\n",
    "        text_tuples.append((data[\"impression\"], {\"doc_key\": f\"{doc_key_prefix}#impression\", \"valid_key\": valid_key}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1136366it [09:47, 1935.78it/s]\n"
     ]
    }
   ],
   "source": [
    "output_file_path = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_reports/raw_reports.json\"\n",
    "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "f = open(output_file_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "doc_tuples = nlp.pipe(text_tuples, as_tuples=True, n_process=8)\n",
    "\n",
    "for doc, info_dict in tqdm(doc_tuples):\n",
    "    output_dict = {\"doc_key\": info_dict[\"doc_key\"], \"sent_toks\": [], \"tok_char_indices\": [], \"sents\": [], \"sent_char_indices\": []}\n",
    "    for sent in doc.sents:\n",
    "        # Remove leading & trailing whitespaces in a sentence\n",
    "        sent_text = sent.text.strip()\n",
    "        sent_start_char = sent.start_char + sent.text.index(sent_text)\n",
    "        sent_end_char = sent_start_char + len(sent_text)\n",
    "        output_dict[\"sents\"].append(sent_text)\n",
    "        output_dict[\"sent_char_indices\"].append((sent_start_char, sent_end_char))\n",
    "\n",
    "        sent_toks = []\n",
    "        tok_char_indices = []\n",
    "        for tok in sent:\n",
    "            tok_text = tok.text.strip()\n",
    "            tok_start_char = tok.idx + tok.text.index(tok_text)\n",
    "            tok_end_char = tok_start_char + len(tok_text)\n",
    "            # Omit empty tokens\n",
    "            if tok_text != \"\":\n",
    "                sent_toks.append(tok_text)\n",
    "                tok_char_indices.append((tok_start_char, tok_end_char))\n",
    "        output_dict[\"sent_toks\"].append(sent_toks)\n",
    "        output_dict[\"tok_char_indices\"].append(tok_char_indices)\n",
    "\n",
    "    f.write(json.dumps(output_dict))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrg_proprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
