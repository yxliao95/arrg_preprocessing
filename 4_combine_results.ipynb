{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/anaconda3/envs/arrg_preprocessing/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Sequence, Image, DatasetDict, concatenate_datasets, Dataset\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union, List\n",
    "import ast\n",
    "import linecache\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonline_from_file(file_path, line_idx):\n",
    "    line = linecache.getline(file_path, line_idx + 1)\n",
    "    return json.loads(line.strip()) if line else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/combined_results\"\n",
    "\n",
    "\n",
    "def save_to_temp(ds, version):\n",
    "    temp_path = os.path.join(temp_dir, f\"temp_v{version}\")\n",
    "    ds.save_to_disk(temp_path)\n",
    "    return temp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load spacy results for reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_file = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_reports/raw_reports.json\"\n",
    "with open(report_file, \"r\") as f:\n",
    "    print(next(f))\n",
    "    print(next(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = Dataset.from_json(report_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load llm-sent-gen results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_file_dir = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/llm_split_sents\"\n",
    "\n",
    "with open(os.path.join(llm_file_dir, \"llm_split_sents_1_of_3.json\"), \"r\") as f:\n",
    "    print(next(f))\n",
    "    print(next(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map = defaultdict(list)\n",
    "\n",
    "for file_idx in range(1, 4):\n",
    "    target_file_path = os.path.join(llm_file_dir, f\"llm_split_sents_{file_idx}_of_3.json\")\n",
    "    with open(target_file_path, \"r\") as f:\n",
    "        for line_idx, line in enumerate(tqdm(f)):\n",
    "            doc = json.loads(line.strip())\n",
    "            doc_map[doc[\"doc_key\"]].append({\"doc_key\": doc[\"doc_key\"], \"split_sent_idx\": int(doc[\"sent_idx\"]), \"file_path\": target_file_path, \"line_idx\": line_idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset(element):\n",
    "    doc_key = element[\"doc_key\"]\n",
    "\n",
    "    sorted_doc_info_list = sorted(doc_map[doc_key], key=lambda x: x[\"split_sent_idx\"])\n",
    "\n",
    "    element[\"split_sents\"] = []\n",
    "    element[\"sent_idx_split_idx\"] = []\n",
    "    for info_dict in sorted_doc_info_list:\n",
    "        # file_doc = {\"doc_key\":\"train#0#impression\",\"sent_idx\":1,\"original_sent\":\"STABLE SMALL LEFT PLEURAL EFFUSION.\",\"split_sents\":[\"Stable small left pleural effusion.\"]}\n",
    "        file_doc = load_jsonline_from_file(info_dict[\"file_path\"], info_dict[\"line_idx\"])\n",
    "        assert element[\"doc_key\"] == file_doc[\"doc_key\"]\n",
    "        assert element[\"sents\"][file_doc[\"sent_idx\"]] == file_doc[\"original_sent\"]\n",
    "\n",
    "        for split_idx, split_sent in enumerate(file_doc[\"split_sents\"]):\n",
    "            if split_sent.strip() == \"\":\n",
    "                continue\n",
    "            element[\"split_sents\"].append(split_sent)\n",
    "            element[\"sent_idx_split_idx\"].append((file_doc[\"sent_idx\"], split_idx))\n",
    "\n",
    "    return element\n",
    "\n",
    "\n",
    "# temp_ds = new_ds.select(range(10))\n",
    "new_ds = new_ds.map(update_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = save_to_temp(new_ds, version=1)\n",
    "temp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load spacy results for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/combined_results/temp_v1\"\n",
    "new_ds = Dataset.load_from_disk(temp_path)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sent_file = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/raw/raw_sents.json\"\n",
    "with open(spacy_sent_file, \"r\") as f:\n",
    "    print(next(f))\n",
    "    print(next(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map = defaultdict(list)\n",
    "\n",
    "with open(spacy_sent_file, \"r\") as f:\n",
    "    for line_idx, line in enumerate(tqdm(f)):\n",
    "        doc = json.loads(line.strip())\n",
    "        data_split, row_idx, section_name, orig_sent_idx, split_sent_idx = doc[\"doc_key\"].split(\"#\")\n",
    "        doc_key = f\"{data_split}#{row_idx}#{section_name}\"\n",
    "\n",
    "        doc_map[doc_key].append({\"doc_key\": doc_key, \"sent_idx\": int(orig_sent_idx), \"split_sent_idx\": int(split_sent_idx), \"file_path\": spacy_sent_file, \"line_idx\": line_idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset(element):\n",
    "    element[\"split_sent_toks\"] = [[] for _ in range(len(element[\"split_sents\"]))]\n",
    "    element[\"split_tok_char_indices\"] = [[] for _ in range(len(element[\"split_sents\"]))]\n",
    "    if len(element[\"split_sents\"]) == 0:\n",
    "        return element\n",
    "\n",
    "    sorted_doc_info_list = sorted(doc_map[element[\"doc_key\"]], key=lambda x: (x[\"sent_idx\"], x[\"split_sent_idx\"]))\n",
    "    for info_dict in sorted_doc_info_list:\n",
    "        # file_doc = {\"doc_key\": \"train#0#impression#0#1\", \"split_sent_text\": \"Decreased bibasilar parenchymal opacities are now minimal.\", \"split_sent_toks\": [[\"Decreased\", \"bibasilar\", \"parenchymal\", \"opacities\", \"are\", \"now\", \"minimal\", \".\"]], \"tok_char_indices\": [[[0, 9], [10, 19], [20, 31], [32, 41], [42, 45], [46, 49], [50, 57], [57, 58]]]}\n",
    "        file_doc = load_jsonline_from_file(info_dict[\"file_path\"], info_dict[\"line_idx\"])\n",
    "        data_split, row_idx, section_name, orig_sent_idx, split_sent_idx = file_doc[\"doc_key\"].split(\"#\")\n",
    "        orig_sent_idx = int(orig_sent_idx)\n",
    "        split_sent_idx = int(split_sent_idx)\n",
    "        assert info_dict[\"sent_idx\"] == orig_sent_idx and info_dict[\"split_sent_idx\"] == split_sent_idx\n",
    "        _doc_key = f\"{data_split}#{row_idx}#{section_name}\"\n",
    "        assert element[\"doc_key\"] == _doc_key\n",
    "        _idx = element[\"sent_idx_split_idx\"].index([orig_sent_idx, split_sent_idx])\n",
    "        assert element[\"split_sents\"][_idx] == file_doc[\"split_sent_text\"]\n",
    "\n",
    "        assert len(file_doc[\"split_sent_toks\"]) == 1\n",
    "        assert len(file_doc[\"tok_char_indices\"]) == 1\n",
    "\n",
    "        element[\"split_sent_toks\"][_idx] = file_doc[\"split_sent_toks\"][0]\n",
    "        element[\"split_tok_char_indices\"][_idx] = file_doc[\"tok_char_indices\"][0]\n",
    "\n",
    "    assert len(element[\"split_sent_toks\"]) == len(element[\"split_sents\"])\n",
    "\n",
    "    return element\n",
    "\n",
    "\n",
    "# temp_ds = new_ds.select(range(10))\n",
    "new_ds = new_ds.map(update_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = save_to_temp(new_ds, version=2)\n",
    "temp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load radlex results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load radlex ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyNode:\n",
    "    def __init__(self, row_idx, class_id, class_name, df_row):\n",
    "        self.row_idx = row_idx\n",
    "        self.class_id = class_id\n",
    "        self.class_name = class_name\n",
    "        self.synonyms = [] if df_row[\"Synonyms\"] == \"\" else df_row[\"Synonyms\"].split(\"|\")\n",
    "        self.df_row = df_row\n",
    "\n",
    "        # The tree structure is maintained by the parent and children attributes. Only one level of parent-child relationship is maintained.\n",
    "        self.parent = []\n",
    "        self.children = []\n",
    "        self.is_root = False\n",
    "        self.tree_level = None\n",
    "\n",
    "        # It's parents from all levels\n",
    "        self._all_parents = []\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def add_parent(self, parent):\n",
    "        self.parent.append(parent)\n",
    "\n",
    "    @property\n",
    "    def all_parents(self):\n",
    "        if self.is_root:\n",
    "            return []\n",
    "        elif self._all_parents:\n",
    "            return self._all_parents\n",
    "        else:\n",
    "            for parent in self.parent:\n",
    "                # 避免父节点重复\n",
    "                self._all_parents = set(parent.all_parents + [parent])\n",
    "                self._all_parents = list(self._all_parents)\n",
    "            return self._all_parents\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, OntologyNode):\n",
    "            return self.class_id == other.class_id\n",
    "        else:\n",
    "            return self.class_id == other\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.class_id)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.class_id}: {self.class_name}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def set_tree_level(curr_node, tree_level):\n",
    "    curr_node.tree_level = tree_level\n",
    "    for child in curr_node.children:\n",
    "        set_tree_level(child, tree_level + 1)\n",
    "    if not curr_node.children:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_radlex_tree(df_csv):\n",
    "    # Build a RadLex node list\n",
    "    node_list = []\n",
    "    root_node = None\n",
    "    for idx, row in tqdm(df_csv.iterrows(), total=df_csv.shape[0], desc=\"Building RadLex tree\"):\n",
    "        ontology_node = OntologyNode(row_idx=idx, class_id=row[\"Class ID\"], class_name=row[\"Preferred Label\"], df_row=row)\n",
    "        if row[\"Preferred Label\"] in row[\"Class ID\"]:\n",
    "            ontology_node.class_name = row[\"http://radlex.org/RID/Preferred_Name_for_Obsolete\"]\n",
    "        node_list.append(ontology_node)\n",
    "\n",
    "    # Resolve the node list and build a RadLex tree\n",
    "    for node in tqdm(node_list, total=len(node_list), desc=\"Building RadLex tree\"):\n",
    "        df_row = node.df_row\n",
    "        parent_ids = df_row[\"Parents\"].split(\"|\")\n",
    "        for parent_id in parent_ids:\n",
    "            parent_row_indices = df_csv.loc[df_csv[\"Class ID\"] == parent_id].index\n",
    "            if not parent_row_indices.empty:\n",
    "                parent_row_idx = parent_row_indices[0]\n",
    "                parent_node = node_list[parent_row_idx]\n",
    "                assert parent_node.class_id == parent_id\n",
    "                node.add_parent(parent_node)\n",
    "                parent_node.add_child(node)\n",
    "            else:\n",
    "                # In radlex, http://radlex.org/RID/RID0 has parent http://www.w3.org/2002/07/owl#Thing.\n",
    "                # However, the RID0 is already the root node in the RadLex ontology. We can safely ignore the owl#Thing.\n",
    "                root_node = node\n",
    "                node.is_root = True\n",
    "                node.tree_level = 0\n",
    "\n",
    "    return node_list, root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radlex_csv_path = \"/home/yuxiang/liao/resources/bioportal/radlex/RADLEX.csv\"\n",
    "df_radlex_csv = pd.read_csv(radlex_csv_path, keep_default_na=False)\n",
    "radlex_nodes, radlex_root_node = build_radlex_tree(df_radlex_csv)\n",
    "radlex_nodes_dict = {node.class_id: node for node in radlex_nodes}\n",
    "print(f\"Number of RadLex nodes: {len(radlex_nodes)}\")\n",
    "\n",
    "# Tracing all parents of nodes\n",
    "for node in radlex_nodes:\n",
    "    node.all_parents\n",
    "\n",
    "set_tree_level(radlex_root_node, tree_level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radlex_file = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/radlex_annotate/radlex_ann.json\"\n",
    "with open(radlex_file, \"r\") as f:\n",
    "    print(next(f))\n",
    "    print(next(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_match_dict = defaultdict(set)\n",
    "fuzzy_match_count = Counter()\n",
    "\n",
    "with open(radlex_file, \"r\") as f:\n",
    "    for line_idx, line in enumerate(tqdm(f)):\n",
    "        doc = json.loads(line.strip())\n",
    "        data_split, row_idx, section_name, orig_sent_idx, split_sent_idx = doc[\"doc_key\"].split(\"#\")\n",
    "        doc_key = f\"{data_split}#{row_idx}#{section_name}\"\n",
    "\n",
    "        position_matches = defaultdict(list)\n",
    "        for matched_info in doc[\"radlex\"]:\n",
    "            # matched_info = {\"match_type\": \"fuzzy_lemma\", \"radlex_id\": \"http://radlex.org/RID/RID5978\", \"radlex_name\": \"parenchyma\", \"matched_text\": \"parenchymal\", \"char_indices\": [20, 31], \"tok_indices\": [2, 3]}\n",
    "            posi_id = \"_\".join(map(str, matched_info[\"tok_indices\"]))\n",
    "            position_matches[posi_id].append(matched_info)\n",
    "\n",
    "        for matched_info in doc[\"radlex\"]:\n",
    "            posi_id = \"_\".join(map(str, matched_info[\"tok_indices\"]))\n",
    "            # 匹配逻辑：id = radlex_id+start+end 如果有exact match，就忽略fuzzy match。但没有考虑不同id的match情况。\n",
    "            # 比如 hemithorax，即能exact match到 hemithorax，也能fuzzy match到 hemothorax\n",
    "            # 我们这里仅分析某个span的所有match都是fuzzy_match\n",
    "            if matched_info[\"match_type\"] == \"fuzzy_lemma\" and all([i[\"match_type\"] == \"fuzzy_lemma\" for i in position_matches[posi_id]]):\n",
    "                fuzzy_match_dict[(matched_info[\"radlex_id\"], matched_info[\"radlex_name\"])].add(matched_info[\"matched_text\"])\n",
    "                fuzzy_match_count.update([(matched_info[\"radlex_id\"], matched_info[\"radlex_name\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fuzzy_match_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in fuzzy_match_count.most_common():\n",
    "    print(k[0])\n",
    "    print(\"  \", k[1], v)\n",
    "    print(\"  \", \", \".join(fuzzy_match_dict[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据 Analyse fuzzy的结果（507个radlex-id），人工筛选出一些不合适的，且出现频率较高的radlex_id。\n",
    "\n",
    "invalid_radlex_ids = set(\n",
    "    [\n",
    "        \"http://radlex.org/RID/RID38667\",  # thinning\n",
    "        \"http://radlex.org/RID/RID5022\",  # stricture\n",
    "        \"http://radlex.org/RID/RID9889\",  # frontalis\n",
    "        \"http://radlex.org/RID/RID3829\",  # scar\n",
    "        \"http://radlex.org/RID/RID5801\",  # lobular\n",
    "        \"http://radlex.org/RID/RID5015\",  # inspissation\n",
    "        \"http://radlex.org/RID/RID5956\",  # contents\n",
    "        \"http://radlex.org/RID/RID5783\",  # contracted\n",
    "        \"http://radlex.org/RID/RID5843\",  # inverted\n",
    "        \"http://radlex.org/RID/RID28656\",  # secretin\n",
    "        \"http://radlex.org/RID/RID35977\",  # property\n",
    "        \"http://radlex.org/RID/RID10453\",  # standing position\n",
    "        \"http://radlex.org/RID/RID43613\",  # Clements view\n",
    "        \"http://radlex.org/RID/RID2198\",  # unciform\n",
    "        \"http://radlex.org/RID/RID49605\",  # training\n",
    "        \"http://radlex.org/RID/RID29980\",  # left hemithorax\n",
    "        \"http://radlex.org/RID/RID29979\",  # right hemithorax\n",
    "        \"http://radlex.org/RID/RID29981\",  # upper hemithorax\n",
    "        \"http://radlex.org/RID/RID29986\",  # left lower hemithorax\n",
    "        \"http://radlex.org/RID/RID29982\",  # right upper hemithorax\n",
    "        \"http://radlex.org/RID/RID29985\",  # right lower hemithorax,\n",
    "        \"http://radlex.org/RID/RID29983\",  # left upper hemithorax\n",
    "    ]\n",
    ")\n",
    "\n",
    "invalid_radlex_text_pairs = {\n",
    "    \"http://radlex.org/RID/RID29984\": \"lower hemothorax\",  # it also has \"lower hemithoraxes\", which is a correect fuzzy match to \"lower hemithorax\"\n",
    "}\n",
    "\n",
    "# 对于 fuzzy 匹配到 hemithorax 和 hemothorax，我们无法判断报告中究竟指的是哪个。因此我们默认报告中的是正确的\n",
    "# 由于 left/right hemithorax 是有精确匹配的，所以只会出现 hemothorax + 位置 被 fuzzy 匹配到的 hemithorax的情况\n",
    "# 当出现这种 fuzzy 匹配时，我们直接忽略，因为我们默认报告中写的 hemothorax 是正确的\n",
    "# （即使不正确，我们也不想要这种数据污染我们的数据集）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radlex_file = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/radlex_annotate/radlex_ann.json\"\n",
    "with open(radlex_file, \"r\") as f:\n",
    "    print(next(f))\n",
    "    print(next(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计不同type下，radlex_id的出现频率。\n",
    "# 在后续处理过程中，当同一个span在同一个type下匹配到多个radlex_id时，我们会选择出现频率最高的radlex_id\n",
    "radlex_freq_dict = {\"text\": Counter(), \"lower_text\": Counter(), \"lemma\": Counter(), \"fuzzy_lemma\": Counter()}\n",
    "\n",
    "with open(radlex_file, \"r\") as f:\n",
    "    for line_idx, line in enumerate(tqdm(f)):\n",
    "        doc = json.loads(line.strip())\n",
    "        data_split, row_idx, section_name, orig_sent_idx, split_sent_idx = doc[\"doc_key\"].split(\"#\")\n",
    "        doc_key = f\"{data_split}#{row_idx}#{section_name}\"\n",
    "\n",
    "        for matched_info in doc[\"radlex\"]:\n",
    "            # matched_info = {\"match_type\": \"fuzzy_lemma\", \"radlex_id\": \"http://radlex.org/RID/RID5978\", \"radlex_name\": \"parenchyma\", \"matched_text\": \"parenchymal\", \"char_indices\": [20, 31], \"tok_indices\": [2, 3]}\n",
    "            radlex_freq_dict[matched_info[\"match_type\"]].update([matched_info[\"radlex_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/combined_results/temp_v2\"\n",
    "new_ds = Dataset.load_from_disk(temp_path)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map = defaultdict(list)\n",
    "\n",
    "with open(radlex_file, \"r\") as f:\n",
    "    for line_idx, line in enumerate(tqdm(f)):\n",
    "        doc = json.loads(line.strip())\n",
    "        data_split, row_idx, section_name, orig_sent_idx, split_sent_idx = doc[\"doc_key\"].split(\"#\")\n",
    "        doc_key = f\"{data_split}#{row_idx}#{section_name}\"\n",
    "\n",
    "        doc_map[doc_key].append({\"doc_key\": doc_key, \"sent_idx\": int(orig_sent_idx), \"split_sent_idx\": int(split_sent_idx), \"file_path\": radlex_file, \"line_idx\": line_idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_priority(element):\n",
    "\n",
    "    element[\"radlex\"] = [[] for _ in range(len(element[\"split_sents\"]))]\n",
    "    if len(element[\"split_sents\"]) == 0:\n",
    "        return element\n",
    "\n",
    "    sorted_doc_info_list = sorted(doc_map[element[\"doc_key\"]], key=lambda x: (x[\"sent_idx\"], x[\"split_sent_idx\"]))\n",
    "    for info_dict in sorted_doc_info_list:\n",
    "        # file_doc = {\"doc_key\": \"train#0#impression#0#1\", \"sent_text\": \"Decreased bibasilar parenchymal opacities are now minimal.\", \"radlex\": [{\"match_type\": \"lemma\", \"radlex_id\": \"http://radlex.org/RID/RID5733\", \"radlex_name\": \"decreasing\", \"matched_text\": \"Decreased\", \"char_indices\": [0, 9], \"tok_indices\": [0, 1]}, ...]}\n",
    "        file_doc = load_jsonline_from_file(info_dict[\"file_path\"], info_dict[\"line_idx\"])\n",
    "        data_split, row_idx, section_name, orig_sent_idx, split_sent_idx = file_doc[\"doc_key\"].split(\"#\")\n",
    "        orig_sent_idx = int(orig_sent_idx)\n",
    "        split_sent_idx = int(split_sent_idx)\n",
    "        assert info_dict[\"sent_idx\"] == orig_sent_idx and info_dict[\"split_sent_idx\"] == split_sent_idx\n",
    "        _doc_key = f\"{data_split}#{row_idx}#{section_name}\"\n",
    "        assert element[\"doc_key\"] == _doc_key\n",
    "        _idx = element[\"sent_idx_split_idx\"].index([orig_sent_idx, split_sent_idx])\n",
    "        assert element[\"split_sents\"][_idx] == file_doc[\"sent_text\"]\n",
    "\n",
    "        position_matches = defaultdict(list)\n",
    "        for matched_info in file_doc[\"radlex\"]:\n",
    "            # matched_info = {\"match_type\": \"fuzzy_lemma\", \"radlex_id\": \"http://radlex.org/RID/RID5978\", \"radlex_name\": \"parenchyma\", \"matched_text\": \"parenchymal\", \"char_indices\": [20, 31], \"tok_indices\": [2, 3]}\n",
    "            posi_id = (matched_info[\"tok_indices\"][0], matched_info[\"tok_indices\"][1])\n",
    "            position_matches[posi_id].append(matched_info)\n",
    "\n",
    "        sorted_position_matches = sorted(position_matches.items(), key=lambda x: x[0])\n",
    "        for _, span_matches in sorted_position_matches:\n",
    "            # 按优先级找到第一个匹配的类型，然后将其加入到radlex中，如果找到后就break，然后进行下一个span（位置）的过滤\n",
    "            for target_type in [\"text\", \"lower_text\", \"lemma\", \"fuzzy_lemma\"]:\n",
    "                # 同一个类型可能会有多个匹配，比如：\n",
    "                # [{'match_type': 'lower_text', 'radlex_id': 'http://radlex.org/RID/RID39433', 'radlex_name': 'arterial phase (liver)', 'matched_text': 'AP', 'char_indices': [0, 2], 'tok_indices': [0, 1]},\n",
    "                # {'match_type': 'lower_text', 'radlex_id': 'http://radlex.org/RID/RID11080', 'radlex_name': 'arterial phase', 'matched_text': 'AP', 'char_indices': [0, 2], 'tok_indices': [0, 1]}]\n",
    "                target_matches = [match for match in span_matches if match[\"match_type\"] == target_type]\n",
    "\n",
    "                # 过滤掉一些无效的radlex_id\n",
    "                filtered_matches = []\n",
    "                for matched_span in target_matches:\n",
    "                    if target_type == \"fuzzy_lemma\":\n",
    "                        if matched_span[\"radlex_id\"] in invalid_radlex_ids:\n",
    "                            continue\n",
    "                        if matched_span[\"radlex_id\"] in invalid_radlex_text_pairs and matched_span[\"matched_text\"] == invalid_radlex_text_pairs[matched_span[\"radlex_id\"]]:\n",
    "                            continue\n",
    "                    filtered_matches.append(matched_span)\n",
    "\n",
    "                # 一个span最多只选择一个radlex_id\n",
    "                # 对于多个匹配，我们根据radlex_id的频率来选择，选择在数据集中出现频率最高的radlex_id\n",
    "                if filtered_matches:\n",
    "                    freqs = [radlex_freq_dict[target_type].get(match[\"radlex_id\"]) for match in filtered_matches]\n",
    "                    matched_span = filtered_matches[freqs.index(max(freqs))]\n",
    "                    element[\"radlex\"][_idx].append(matched_span)\n",
    "                    break\n",
    "\n",
    "    return element\n",
    "\n",
    "\n",
    "# temp_ds = new_ds.select(range(10))\n",
    "new_ds = new_ds.map(filter_by_priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds[37][\"split_sents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds[37][\"radlex\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = save_to_temp(new_ds, version=1)\n",
    "temp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cxrgraph results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/combined_results/temp_v1\"\n",
    "new_ds = Dataset.load_from_disk(temp_path)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxrgraph_file = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/cxrgraph/inference.json\"\n",
    "\n",
    "with open(cxrgraph_file, \"r\") as f:\n",
    "    print(next(f))\n",
    "    print(next(f))\n",
    "    print(next(f))\n",
    "    print(next(f))\n",
    "    print(next(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map = defaultdict(list)\n",
    "\n",
    "with open(cxrgraph_file, \"r\") as f:\n",
    "    for line_idx, line in enumerate(tqdm(f)):\n",
    "        doc = json.loads(line.strip())\n",
    "        data_split, row_idx, section_name, orig_sent_idx, split_sent_idx = doc[\"doc_key\"].split(\"#\")\n",
    "        doc_key = f\"{data_split}#{row_idx}#{section_name}\"\n",
    "\n",
    "        doc_map[doc_key].append({\"doc_key\": doc_key, \"sent_idx\": int(orig_sent_idx), \"split_sent_idx\": int(split_sent_idx), \"file_path\": cxrgraph_file, \"line_idx\": line_idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_all_number_or_symbols(lst):\n",
    "    return all(all(char in \"!@#$%^&*()-_=+[]{};:'\\\",.<>?/|\\\\`~\" or char.isdigit() for char in item) for item in lst)\n",
    "\n",
    "\n",
    "def update_dataset(element):\n",
    "    element[\"cxrgraph_ent\"] = [[] for _ in range(len(element[\"split_sents\"]))]\n",
    "    element[\"cxrgraph_attr\"] = [[] for _ in range(len(element[\"split_sents\"]))]\n",
    "    element[\"cxrgraph_rel\"] = [[] for _ in range(len(element[\"split_sents\"]))]\n",
    "    if len(element[\"split_sents\"]) == 0:\n",
    "        return element\n",
    "\n",
    "    sorted_doc_info_list = sorted(doc_map[element[\"doc_key\"]], key=lambda x: (x[\"sent_idx\"], x[\"split_sent_idx\"]))\n",
    "    for info_dict in sorted_doc_info_list:\n",
    "        # file_doc = {\"doc_key\": \"train#0#impression#0#1\", \"sentences\": [[\"Decreased\", \"bibasilar\", \"parenchymal\", \"opacities\", \"are\", \"now\", \"minimal\", \".\"]],\n",
    "        # \"pred_ner\": [[[0, 0, \"Observation-Present\"], [1, 1, \"Anatomy\"], [2, 2, \"Anatomy\"], [3, 3, \"Observation-Present\"], [6, 6, \"Observation-Present\"], [7, 7, \"Observation-Present\"]]],\n",
    "        # \"pred_attr\": [[[0, 0, \"NA\", \"NA\", \"NA\"]]],\n",
    "        # \"pred_rel\": [[[0, 0, 3, 3, \"modify\"], [2, 2, 1, 1, \"part_of\"], [3, 3, 2, 2, \"located_at\"], [6, 6, 7, 7, \"modify\"], [6, 6, 3, 3, \"modify\"], [7, 7, 3, 3, \"modify\"]]]}\n",
    "        file_doc = load_jsonline_from_file(info_dict[\"file_path\"], info_dict[\"line_idx\"])\n",
    "        data_split, row_idx, section_name, orig_sent_idx, split_sent_idx = file_doc[\"doc_key\"].split(\"#\")\n",
    "        orig_sent_idx = int(orig_sent_idx)\n",
    "        split_sent_idx = int(split_sent_idx)\n",
    "        assert info_dict[\"sent_idx\"] == orig_sent_idx and info_dict[\"split_sent_idx\"] == split_sent_idx\n",
    "        _doc_key = f\"{data_split}#{row_idx}#{section_name}\"\n",
    "        assert element[\"doc_key\"] == _doc_key\n",
    "        _idx = element[\"sent_idx_split_idx\"].index([orig_sent_idx, split_sent_idx])\n",
    "        assert element[\"split_sent_toks\"][_idx] == file_doc[\"sentences\"][0]\n",
    "\n",
    "        assert len(file_doc[\"sentences\"]) == 1\n",
    "        sent_text = file_doc[\"sentences\"][0]\n",
    "        assert sent_text == element[\"split_sent_toks\"][_idx]\n",
    "\n",
    "        for ner in file_doc[\"pred_ner\"][0]:\n",
    "            # ner = [0, 0, \"Observation-Present\"]\n",
    "            tok_start = ner[0]\n",
    "            tok_end = ner[1] + 1\n",
    "            ent_toks = sent_text[tok_start:tok_end]\n",
    "            if not is_all_number_or_symbols(ent_toks):\n",
    "                ent_type = ner[2]\n",
    "                element[\"cxrgraph_ent\"][_idx].append({\"tok_indices\": [tok_start, tok_end], \"ent_toks\": ent_toks, \"ent_type\": ent_type})\n",
    "\n",
    "        for attr in file_doc[\"pred_attr\"][0]:\n",
    "            # attr = [0, 0, \"NA\", \"NA\", \"NA\"]\n",
    "            tok_start = attr[0]\n",
    "            tok_end = attr[1] + 1\n",
    "            ent_toks = sent_text[tok_start:tok_end]\n",
    "            if not is_all_number_or_symbols(ent_toks):\n",
    "                attr_normality = attr[2]\n",
    "                attr_action = attr[3]\n",
    "                attr_change = attr[4]\n",
    "                if attr_normality != \"NA\" or attr_action != \"NA\" or attr_change != \"NA\":\n",
    "                    element[\"cxrgraph_attr\"][_idx].append({\"tok_indices\": [tok_start, tok_end], \"ent_toks\": ent_toks, \"attr_normality\": attr_normality, \"attr_action\": attr_action, \"attr_change\": attr_change})\n",
    "\n",
    "        for rel in file_doc[\"pred_rel\"][0]:\n",
    "            # rel = [0, 0, 3, 3, \"modify\"]\n",
    "            subj_tok_start = rel[0]\n",
    "            subj_tok_end = rel[1] + 1\n",
    "            subj_toks = sent_text[subj_tok_start:subj_tok_end]\n",
    "            obj_tok_start = rel[2]\n",
    "            obj_tok_end = rel[3] + 1\n",
    "            obj_toks = sent_text[obj_tok_start:obj_tok_end]\n",
    "            if not is_all_number_or_symbols(subj_toks) and not is_all_number_or_symbols(obj_toks):\n",
    "                rel_type = rel[4]\n",
    "                element[\"cxrgraph_rel\"][_idx].append({\"subj_tok_indices\": [subj_tok_start, subj_tok_end], \"subj_toks\": subj_toks, \"obj_tok_indices\": [obj_tok_start, obj_tok_end], \"obj_toks\": obj_toks, \"rel_type\": rel_type})\n",
    "\n",
    "    return element\n",
    "\n",
    "\n",
    "# temp_ds = new_ds.select(range(100))\n",
    "new_ds = new_ds.map(update_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = save_to_temp(new_ds, version=2)\n",
    "temp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load radcoref results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/combined_results/temp_v2\"\n",
    "new_ds = Dataset.load_from_disk(temp_path)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxrgraph_file = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/radcoref/coref_inference.json\"\n",
    "\n",
    "with open(cxrgraph_file, \"r\") as f:\n",
    "    print(next(f))\n",
    "    next(f)\n",
    "    next(f)\n",
    "    next(f)\n",
    "    next(f)\n",
    "    print(next(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map = {}\n",
    "\n",
    "with open(cxrgraph_file, \"r\") as f:\n",
    "    for line_idx, line in enumerate(tqdm(f)):\n",
    "        doc = json.loads(line.strip())\n",
    "        doc_key = doc[\"doc_key\"]\n",
    "\n",
    "        doc_map[doc_key] = {\"doc_key\": doc_key, \"file_path\": cxrgraph_file, \"line_idx\": line_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset(element):\n",
    "    element[\"radcoref\"] = []\n",
    "    if len(element[\"split_sents\"]) == 0:\n",
    "        return element\n",
    "\n",
    "    # file_doc = {\"doc_key\": \"train#0#impression\", \"split_sent_toks\": [[\"12/15/2004\", \"at\", \"1830\", \"accession\", \":\", \"6772908\", \":\"], [\"Stable\", \"right\", \"pigtail\", \"pleural\", \"catheter\", \".\"], [\"ETT\", \"is\", \"stable\", \".\"], ...],\n",
    "    # \"coref_clusters\": [[[0, 0, \"12/15/2004\"]], [[13, 13, \"ETT\"], [143, 144, \"Stable ETT\"]], [[17, 19, \"The NG tube\"], [146, 147, \"NG tube\"]], [[30, 33, \"Right upper lobe opacity\"], [35, 36, \"The opacity\"], [92, 95, \"the right upper lobe\"], [116, 116, \"Opacity\"], [123, 124, \"The opacity\"], [162, 165, \"the right upper lobe\"], [167, 168, \"The opacity\"]], [[49, 55, \"Fracture of the posterior right rib .\"], [49, 54, \"Fracture of the posterior right rib\"], [56, 57, \"The fracture\"]], [[76, 76, \"CT\"], [88, 88, \"CT\"]]]}\n",
    "    info_dict = doc_map[element[\"doc_key\"]]\n",
    "    file_doc = load_jsonline_from_file(info_dict[\"file_path\"], info_dict[\"line_idx\"])\n",
    "\n",
    "    assert element[\"doc_key\"] == file_doc[\"doc_key\"]\n",
    "    # 注意，我们在用spacy处理llm-sent时，会手动过滤不合格的sent。所以被过滤的句子对应的split_sent_toks会是空list\n",
    "    for test_idx, test_toks in enumerate(file_doc[\"split_sent_toks\"]):\n",
    "        if test_toks != []:\n",
    "            assert element[\"split_sent_toks\"][test_idx] == test_toks\n",
    "\n",
    "    toks = []\n",
    "    tok2sent_map = []\n",
    "    for sent_idx, sent in enumerate(element[\"split_sent_toks\"]):\n",
    "        for tok in sent:\n",
    "            toks.append(tok)\n",
    "            tok2sent_map.append(sent_idx)\n",
    "\n",
    "    for cluster in file_doc[\"coref_clusters\"]:\n",
    "        if len(cluster) > 1:\n",
    "            out_cluster = []\n",
    "            for mention in cluster:\n",
    "                # mention = [0, 0, \"12/15/2004\"]\n",
    "                tok_start = mention[0]\n",
    "                tok_end = mention[1] + 1\n",
    "\n",
    "                mention_toks = toks[tok_start:tok_end]\n",
    "                assert mention[2] == \" \".join(mention_toks)\n",
    "\n",
    "                target_sent_idx = tok2sent_map[tok_start]\n",
    "                out_cluster.append({\"tok_indices\": [tok_start, tok_end], \"mention_toks\": mention_toks, \"target_sent_idx\": target_sent_idx})\n",
    "            element[\"radcoref\"].append(out_cluster)\n",
    "\n",
    "    return element\n",
    "\n",
    "\n",
    "# temp_ds = new_ds.select(range(100))\n",
    "new_ds = new_ds.map(update_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = save_to_temp(new_ds, version=1)\n",
    "temp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doc_key', 'sent_toks', 'tok_char_indices', 'sents', 'sent_char_indices', 'split_sents', 'sent_idx_split_idx', 'split_sent_toks', 'split_tok_char_indices', 'radlex', 'cxrgraph_ent', 'cxrgraph_attr', 'cxrgraph_rel', 'radcoref'],\n",
       "    num_rows: 1136366\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_path = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/combined_results/temp_v1\"\n",
    "new_ds = Dataset.load_from_disk(temp_path)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in tqdm(new_ds):\n",
    "    if any([len(sent) == 0 for sent in data[\"split_sent_toks\"]]):\n",
    "        print(data[\"doc_key\"])\n",
    "        print(data[\"split_sents\"])\n",
    "        print(data[\"split_sent_toks\"])\n",
    "        break\n",
    "\n",
    "# 确认 split_sents 中的invalid sents 会被忽略，导致 split_sent_toks 中对应index 的位置是空list\n",
    "# 这个情况不需要处理，因为radcoref中的每个mention都明确的指明了split_sent_index。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (13/13 shards): 100%|██████████| 1136366/1136366 [00:16<00:00, 67942.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "new_ds = new_ds.save_to_disk(\"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/combined_results/combine_final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrg_preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
