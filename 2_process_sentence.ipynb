{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret-cxr sentences\n",
    "\n",
    "After getting the split sentences from `arrg_sentgen`, we need to process it for `bioportal`, `cxrgraph` and `radcoref`\n",
    "\n",
    "We take LLM output as input. Empty reports (sections) have been omitted from the input\n",
    "\n",
    "Example input:\n",
    "\n",
    "```\n",
    "{\"doc_key\": \"train#2#findings\",\n",
    " \"sent_idx\": 0,\n",
    " \"original_sent\": \"Normal cardiomedastinal silhouette without evidence of pulmonary infiltrates or occupation of the costophrenic sinuses.\",\n",
    " \"split_sents\": [\"Normal cardiomedastinal silhouette.\",\"No evidence of pulmonary infiltrates.\",\"No occupation of the costophrenic sinuses.\"]}\n",
    "```\n",
    "\n",
    "Example output:\n",
    "\n",
    "```\n",
    "{\"doc_key\": \"train#0#impression#0#0\",\n",
    " \"split_sent_text\": \"Decreased bibasilar parenchymal opacities are seen.\",\n",
    " \"split_sent_toks\": [[\"Decreased\", \"bibasilar\", \"parenchymal\", \"opacities\", \"are\", \"seen\", \".\"]], \n",
    " \"tok_char_indices\": [[[0, 9], [10, 19], [20, 31], [32, 41], [42, 45], [46, 50], [50, 51]]]}\n",
    " ```\n",
    "\n",
    "All items are not empty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_sents_dir = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/llm_split_sents\"\n",
    "\n",
    "docs = []\n",
    "for partition in [1, 2, 3]:\n",
    "    with open(os.path.join(llm_sents_dir, f\"llm_split_sents_{partition}_of_3.json\")) as f:\n",
    "        docs += [json.loads(line.strip()) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'parser']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"tagger\", \"attribute_ruler\", \"lemmatizer\", \"ner\"])\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2872770 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2872770/2872770 [00:09<00:00, 300360.53it/s]\n"
     ]
    }
   ],
   "source": [
    "text_tuples = []\n",
    "for idx, doc in enumerate(tqdm(docs)):\n",
    "    doc_key_prefix = f'{doc[\"doc_key\"]}#{doc[\"sent_idx\"]}'\n",
    "    for split_sent_idx, split_sent_text in enumerate(doc[\"split_sents\"]):\n",
    "        text_tuples.append((split_sent_text, {\"data_id\": f\"{doc_key_prefix}#{split_sent_idx}\", **doc}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5335339/5335339 [27:46<00:00, 3200.87it/s] \n"
     ]
    }
   ],
   "source": [
    "output_file_dir = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/raw\"\n",
    "os.makedirs(output_file_dir, exist_ok=True)\n",
    "f = open(os.path.join(output_file_dir, \"raw_sents.json\"), \"w\", encoding=\"utf-8\")\n",
    "f_warn_empty = open(os.path.join(output_file_dir, \"warn_empty_sent.json\"), \"w\", encoding=\"utf-8\")\n",
    "f_warn_multi = open(os.path.join(output_file_dir, \"warn_multi_sent.json\"), \"w\", encoding=\"utf-8\")\n",
    "f_warn_inval = open(os.path.join(output_file_dir, \"warn_inval_sent.json\"), \"w\", encoding=\"utf-8\")\n",
    "\n",
    "remove_if_contain = [\"You are acting as a radiologist assistant\", \"I'm ready to assist you\", \"There is no input text provided\", \"Please provide the sentence\"]\n",
    "\n",
    "# each return is a split sentence\n",
    "for doc, info_dict in tqdm(nlp.pipe(text_tuples, as_tuples=True, n_process=8), total=len(text_tuples)):\n",
    "    doc_sents = list(doc.sents)\n",
    "    if len(doc_sents) == 0:\n",
    "        # The split sentence should not be empty, as we did not provide empty sentences\n",
    "        # Because we ignore the empty sentences, the data_id may not be continuous\n",
    "        f_warn_empty.write(json.dumps({\"nlp\": [[tok.text for tok in sent] for sent in doc_sents], **info_dict}))\n",
    "        f_warn_empty.write(\"\\n\")\n",
    "    elif any([invalid_sent in \" \".join(info_dict[\"split_sents\"]) for invalid_sent in remove_if_contain]):\n",
    "        # The split sentence should not contain invalid sentences, as sentences that are unrecognized by llm should remain the same.\n",
    "        f_warn_inval.write(json.dumps({\"nlp\": [[tok.text for tok in sent] for sent in doc.sents], **info_dict}))\n",
    "        f_warn_inval.write(\"\\n\")\n",
    "    else:\n",
    "        output_dict = {\"doc_key\": info_dict[\"data_id\"], \"split_sent_text\": doc.text, \"split_sent_toks\": [], \"tok_char_indices\": []}\n",
    "        # Should only have one sentence\n",
    "        # If there are multipel sentences, we treated it as one sentence, and record it as a warning\n",
    "        if len(doc_sents) > 1:\n",
    "            f_warn_multi.write(json.dumps({\"nlp\": [[tok.text for tok in sent] for sent in doc.sents], **info_dict}))\n",
    "            f_warn_multi.write(\"\\n\")\n",
    "\n",
    "        split_sent_toks = []\n",
    "        tok_char_indices = []\n",
    "        # All tokens are put into the same sentence\n",
    "        for tok in doc:\n",
    "            tok_text = tok.text.strip()\n",
    "            tok_start_char = tok.idx + tok.text.index(tok_text)\n",
    "            tok_end_char = tok_start_char + len(tok_text)\n",
    "            # Omit empty tokens\n",
    "            if tok_text != \"\":\n",
    "                split_sent_toks.append(tok_text)\n",
    "                tok_char_indices.append((tok_start_char, tok_end_char))\n",
    "\n",
    "        assert len(split_sent_toks) != 0\n",
    "        output_dict[\"split_sent_toks\"].append(split_sent_toks)\n",
    "        output_dict[\"tok_char_indices\"].append(tok_char_indices)\n",
    "        f.write(json.dumps(output_dict))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5330591\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(output_file_dir, \"raw_sents.json\"), \"rb\") as f:\n",
    "    print(sum(1 for _ in f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrg_proprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
