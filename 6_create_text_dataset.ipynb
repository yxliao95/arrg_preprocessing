{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/anaconda3/envs/arrg_preprocessing/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Sequence, Image, DatasetDict, concatenate_datasets, Dataset\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union, List\n",
    "import ast\n",
    "import linecache\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyNode:\n",
    "    def __init__(self, row_idx, class_id, class_name, df_row):\n",
    "        self.row_idx = row_idx\n",
    "        self.class_id = class_id\n",
    "        self.class_name = class_name\n",
    "        self.synonyms = [] if df_row[\"Synonyms\"] == \"\" else df_row[\"Synonyms\"].split(\"|\")\n",
    "        self.df_row = df_row\n",
    "\n",
    "        # The tree structure is maintained by the parent and children attributes. Only one level of parent-child relationship is maintained.\n",
    "        self.parent = []\n",
    "        self.children = []\n",
    "        self.is_root = False\n",
    "        self.tree_level = None\n",
    "\n",
    "        # It's parents from all levels\n",
    "        self._all_parents = []\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def add_parent(self, parent):\n",
    "        self.parent.append(parent)\n",
    "\n",
    "    @property\n",
    "    def all_parents(self):\n",
    "        if self.is_root:\n",
    "            return []\n",
    "        elif self._all_parents:\n",
    "            return self._all_parents\n",
    "        else:\n",
    "            for parent in self.parent:\n",
    "                # 避免父节点重复\n",
    "                self._all_parents = set(parent.all_parents + [parent])\n",
    "                self._all_parents = list(self._all_parents)\n",
    "            return self._all_parents\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, OntologyNode):\n",
    "            return self.class_id == other.class_id\n",
    "        else:\n",
    "            return self.class_id == other\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.class_id)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.class_id}: {self.class_name}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def set_tree_level(curr_node, tree_level):\n",
    "    curr_node.tree_level = tree_level\n",
    "    for child in curr_node.children:\n",
    "        set_tree_level(child, tree_level + 1)\n",
    "    if not curr_node.children:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_radlex_tree(df_csv):\n",
    "    # Build a RadLex node list\n",
    "    node_list = []\n",
    "    root_node = None\n",
    "    for idx, row in tqdm(df_csv.iterrows(), total=df_csv.shape[0], desc=\"Building RadLex tree\"):\n",
    "        ontology_node = OntologyNode(row_idx=idx, class_id=row[\"Class ID\"], class_name=row[\"Preferred Label\"], df_row=row)\n",
    "        if row[\"Preferred Label\"] in row[\"Class ID\"]:\n",
    "            ontology_node.class_name = row[\"http://radlex.org/RID/Preferred_Name_for_Obsolete\"]\n",
    "        node_list.append(ontology_node)\n",
    "\n",
    "    # Resolve the node list and build a RadLex tree\n",
    "    for node in tqdm(node_list, total=len(node_list), desc=\"Building RadLex tree\"):\n",
    "        df_row = node.df_row\n",
    "        parent_ids = df_row[\"Parents\"].split(\"|\")\n",
    "        for parent_id in parent_ids:\n",
    "            parent_row_indices = df_csv.loc[df_csv[\"Class ID\"] == parent_id].index\n",
    "            if not parent_row_indices.empty:\n",
    "                parent_row_idx = parent_row_indices[0]\n",
    "                parent_node = node_list[parent_row_idx]\n",
    "                assert parent_node.class_id == parent_id\n",
    "                node.add_parent(parent_node)\n",
    "                parent_node.add_child(node)\n",
    "            else:\n",
    "                # In radlex, http://radlex.org/RID/RID0 has parent http://www.w3.org/2002/07/owl#Thing.\n",
    "                # However, the RID0 is already the root node in the RadLex ontology. We can safely ignore the owl#Thing.\n",
    "                root_node = node\n",
    "                node.is_root = True\n",
    "                node.tree_level = 0\n",
    "\n",
    "    return node_list, root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building RadLex tree: 100%|██████████| 46761/46761 [00:02<00:00, 22832.44it/s]\n",
      "Building RadLex tree: 100%|██████████| 46761/46761 [01:10<00:00, 665.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RadLex nodes: 46761\n"
     ]
    }
   ],
   "source": [
    "radlex_csv_path = \"/home/yuxiang/liao/resources/bioportal/radlex/RADLEX.csv\"\n",
    "df_radlex_csv = pd.read_csv(radlex_csv_path, keep_default_na=False)\n",
    "radlex_nodes, radlex_root_node = build_radlex_tree(df_radlex_csv)\n",
    "radlex_nodes_dict = {node.class_id: node for node in radlex_nodes}\n",
    "print(f\"Number of RadLex nodes: {len(radlex_nodes)}\")\n",
    "\n",
    "# Tracing all parents of nodes\n",
    "for node in radlex_nodes:\n",
    "    node.all_parents\n",
    "\n",
    "set_tree_level(radlex_root_node, tree_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://radlex.org/RID/RID35591: string-of-pearls sign of bowel\n",
      "[http://radlex.org/RID/RID29023: imaging sign]\n",
      "[http://radlex.org/RID/RID29023: imaging sign, http://radlex.org/RID/RID5: imaging observation, http://radlex.org/RID/RID1: RadLex entity, http://radlex.org/RID/RID0: RadLex ontology entity]\n"
     ]
    }
   ],
   "source": [
    "print(radlex_nodes[0])\n",
    "print(radlex_nodes[0].parent)\n",
    "print(sorted(radlex_nodes[0].all_parents, key=lambda x: x.tree_level, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-orgainze dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从effusion开始: http://radlex.org/RID/RID4872\n",
    "\n",
    "仅保留doc中包含effusion的split_sent\n",
    "\n",
    "探索这些句子所包含的graph模式\n",
    "\n",
    "探索句子的共指\n",
    "\n",
    "探索句子的attr对训练是否有影响\n",
    "\n",
    "按照img dataset的格式重新组织数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_sents/combined_results/combine_final\"\n",
    "ds_text = datasets.load_from_disk(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标是获取图像的同时也能获取文字\n",
    "但如果要把文字合并到数据集中的话性能不够\n",
    "如果文字单独保存的话，要怎么读取？\n",
    "比如用selectid\n",
    "\n",
    "可以先读取text数据集，然后用doc_key解析出img数据集，然后用select选择图像\n",
    "可行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1136366/1136366 [10:34<00:00, 1791.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# radlex节点与doc_key的映射\n",
    "\n",
    "radlex_dockey_dict = defaultdict(list)\n",
    "radlex_sentkey_dict = defaultdict(list)\n",
    "\n",
    "for data_row in tqdm(ds_text):\n",
    "    doc_key = data_row[\"doc_key\"]  # train#0#impression\n",
    "    for sent_idx, sent in enumerate(data_row[\"radlex\"]):\n",
    "        for radlex_item in sent:\n",
    "            # {\"char_indices\": [20, 31], \"match_type\": \"fuzzy_lemma\", \"matched_text\": \"parenchymal\", \"radlex_id\": \"http://radlex.org/RID/RID5978\", \"radlex_name\": \"parenchyma\", \"tok_indices\": [2, 3]}\n",
    "            radlex_node = radlex_nodes_dict[radlex_item[\"radlex_id\"]]\n",
    "            radlex_dockey_dict[radlex_node].append(doc_key)\n",
    "            radlex_sentkey_dict[radlex_node].append((doc_key, sent_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1136366/1136366 [00:09<00:00, 124860.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# 处理后的文本数据集与原始的图像数据集的映射\n",
    "doc_key_map = {\n",
    "    \"findings\": {\"train\": {}, \"validation\": {}, \"test\": {}},\n",
    "    \"impression\": {\"train\": {}, \"validation\": {}, \"test\": {}},\n",
    "}\n",
    "\n",
    "for textDs_row_idx, data_row in enumerate(tqdm(ds_text.select_columns([\"doc_key\"]))):\n",
    "    doc_key = data_row[\"doc_key\"]  # train#0#impression\n",
    "    data_split, imgDs_row_idx, section_name = doc_key.split(\"#\")\n",
    "\n",
    "    doc_key_map[section_name][data_split][imgDs_row_idx] = textDs_row_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取目标radlex节点所涉及的的doc_keys\n",
    "\n",
    "inclusive = {\n",
    "    \"http://radlex.org/RID/RID5\": \"imaging observation\",\n",
    "    \"http://radlex.org/RID/RID34785\": \"clinical finding\",\n",
    "    \"http://radlex.org/RID/RID34861\": \"object\",\n",
    "    \"http://radlex.org/RID/RID1559\": \"procedure\",\n",
    "    \"http://radlex.org/RID/RID35977\": \"property\",\n",
    "    \"http://radlex.org/RID/RID3\": \"anatomical entity\",\n",
    "    \"http://radlex.org/RID/RID6\": \"RadLex descriptor\",\n",
    "}\n",
    "\n",
    "# 过滤，仅保留inclusive相关的radlex节点\n",
    "node_dockey_dict = defaultdict(set)\n",
    "for node, dockeys in radlex_dockey_dict.items():\n",
    "    # Check if the node or its parents is in the inclusive list\n",
    "    if any([cls_id in node.all_parents or cls_id == node.class_id for cls_id in inclusive.keys()]):\n",
    "        node_dockey_dict[node].update(dockeys)\n",
    "\n",
    "nodes = [node for node in node_dockey_dict.keys()]\n",
    "dockey_sets = [set(ids) for ids in node_dockey_dict.values()]\n",
    "\n",
    "# 按照node在报告中出现的次数排序\n",
    "# 如果node的父节点已经出现过，那么就不再单独统计，而是将其涉及的doc_keys传递给父节点，并跳过当前node\n",
    "# 如果node的父节点有多个被添加进统计中，则选择tree_level最近的父节点，比如当前是level4，则选择level3的父节点\n",
    "aggregrated_nodes, aggregrated_dockey_sets = [], []\n",
    "for node, key_set in sorted(zip(nodes, dockey_sets), key=lambda x: len(x[1]), reverse=True):\n",
    "    is_parent_exist = False\n",
    "    for parent_node in sorted(node.all_parents, key=lambda x: x.tree_level, reverse=True):\n",
    "        if parent_node in aggregrated_nodes:\n",
    "            idx = aggregrated_nodes.index(parent_node)\n",
    "            aggregrated_dockey_sets[idx].update(key_set)\n",
    "            is_parent_exist = True\n",
    "            break\n",
    "\n",
    "    if not is_parent_exist:\n",
    "        aggregrated_nodes.append(node)\n",
    "        aggregrated_dockey_sets.append(key_set)\n",
    "\n",
    "nodes, dockey_sets = zip(*sorted(zip(aggregrated_nodes, aggregrated_dockey_sets), key=lambda x: len(x[1]), reverse=True))\n",
    "assert len(nodes) == len(dockey_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3279\n",
      "1278\n"
     ]
    }
   ],
   "source": [
    "# 仅保留inclusive相关的radlex节点\n",
    "print(len(node_dockey_dict))\n",
    "\n",
    "# 按照node在报告中出现的次数排序\n",
    "# 如果node的父节点已经出现过，那么就不再单独统计，而是将其涉及的doc_keys传递给父节点，并跳过当前node\n",
    "# 如果node的父节点有多个被添加进统计中，则选择tree_level最近的父节点，比如当前是level4，则选择level3的父节点\n",
    "# 具体见 5_radlex_ontology.py 合并数据\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of total involved report sections: 733093\n"
     ]
    }
   ],
   "source": [
    "target_dockey_set = set([doc_key for doc_keys in dockey_sets for doc_key in doc_keys])\n",
    "print(f\"Num of total involved report sections: {len(target_dockey_set)}\")\n",
    "\n",
    "\n",
    "def ds_generator(target_section_name, target_data_split):\n",
    "    for doc_key in target_dockey_set:\n",
    "        data_split, imgDs_row_idx, section_name = doc_key.split(\"#\")\n",
    "\n",
    "        if data_split != target_data_split or section_name != target_section_name:\n",
    "            continue\n",
    "\n",
    "        ds_text_row_idx = doc_key_map[section_name][data_split][imgDs_row_idx]\n",
    "        ds_text_data_row = ds_text[ds_text_row_idx]\n",
    "        # ds_text_data_row: dict_keys(['doc_key', 'sent_toks', 'tok_char_indices', 'sents', 'sent_char_indices', 'split_sents', 'sent_idx_split_idx', 'split_sent_toks', 'split_tok_char_indices', 'radlex', 'cxrgraph_ent', 'cxrgraph_attr', 'cxrgraph_rel', 'radcoref'])\n",
    "\n",
    "        if ds_text_data_row[\"sents\"] == []:\n",
    "            continue\n",
    "\n",
    "        # 不需要任何过滤 radlex 节点。使用所有内容\n",
    "        output_data_row = {\n",
    "            \"doc_key\": doc_key,\n",
    "            \"sents\": ds_text_data_row[\"sents\"],\n",
    "            \"sent_toks\": ds_text_data_row[\"sent_toks\"],\n",
    "            \"tok_char_indices\": ds_text_data_row[\"tok_char_indices\"],\n",
    "            \"split_sents\": ds_text_data_row[\"split_sents\"],\n",
    "            \"split_sent_toks\": ds_text_data_row[\"split_sent_toks\"],\n",
    "            \"sent_idx_split_idx\": ds_text_data_row[\"sent_idx_split_idx\"],\n",
    "            \"radlex\": ds_text_data_row[\"radlex\"],\n",
    "            \"cxrgraph_ent\": ds_text_data_row[\"cxrgraph_ent\"],\n",
    "            \"cxrgraph_attr\": ds_text_data_row[\"cxrgraph_attr\"],\n",
    "            \"cxrgraph_rel\": ds_text_data_row[\"cxrgraph_rel\"],\n",
    "            \"radcoref\": ds_text_data_row[\"radcoref\"],\n",
    "        }\n",
    "\n",
    "        assert len(output_data_row[\"split_sents\"]) != 0\n",
    "\n",
    "        yield output_data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 343738 examples [13:03, 438.94 examples/s]\n",
      "Generating train split: 8825 examples [00:20, 432.38 examples/s]\n",
      "Generating train split: 2692 examples [00:09, 291.89 examples/s]\n",
      "Saving the dataset (6/6 shards): 100%|██████████| 343738/343738 [00:01<00:00, 228124.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8825/8825 [00:00<00:00, 223079.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2692/2692 [00:00<00:00, 136158.34 examples/s]\n",
      "Generating train split: 365565 examples [12:17, 495.45 examples/s] \n",
      "Generating train split: 9308 examples [00:18, 495.19 examples/s] \n",
      "Generating train split: 2965 examples [00:05, 578.91 examples/s] \n",
      "Saving the dataset (6/6 shards): 100%|██████████| 365565/365565 [00:01<00:00, 251863.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 9308/9308 [00:00<00:00, 211098.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2965/2965 [00:00<00:00, 190393.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_text/all\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for section_name in [\"findings\", \"impression\"]:\n",
    "    ds_dict = {}\n",
    "    for data_split in [\"train\", \"validation\", \"test\"]:\n",
    "        ds_dict[data_split] = Dataset.from_generator(ds_generator, gen_kwargs={\"target_section_name\": section_name, \"target_data_split\": data_split})\n",
    "    dataset_dict_final = DatasetDict(ds_dict)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"interpret_text_{section_name}\")\n",
    "    dataset_dict_final.save_to_disk(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on radlex node: \"effusion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新构造一个文本数据子集，\n",
    "只保留目标split_sent，dockey，cxrgraphs，radlex\n",
    "\n",
    "radcoref用于协助句子选取\n",
    "\n",
    "现在的主要问题是需要把radcoref用上。\n",
    "\n",
    "比如我用radlex，那么这几个radlex就视为coref。但实际上不是，因为左右有区别。\n",
    "但目前的任务暂时用不上，因为我们不区分左右。只考虑effusion。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_node_idx = nodes.index(\"http://radlex.org/RID/RID4872\")\n",
    "target_node = nodes[target_node_idx]\n",
    "target_dockey_set = dockey_sets[target_node_idx]\n",
    "print(f\"Target node: {target_node.class_name} ({len(target_dockey_set)} report sections)\")\n",
    "\n",
    "\n",
    "def ds_generator(target_section_name, target_data_split, target_radlex_node_id):\n",
    "    for doc_key in target_dockey_set:\n",
    "        data_split, imgDs_row_idx, section_name = doc_key.split(\"#\")\n",
    "\n",
    "        if data_split != target_data_split or section_name != target_section_name:\n",
    "            continue\n",
    "\n",
    "        ds_text_row_idx = doc_key_map[section_name][data_split][imgDs_row_idx]\n",
    "        ds_text_data_row = ds_text[ds_text_row_idx]\n",
    "\n",
    "        output_data_row = {\n",
    "            \"doc_key\": doc_key,\n",
    "            \"split_sents\": [],\n",
    "            \"split_sent_toks\": [],\n",
    "            \"sent_idx_split_idx\": [],\n",
    "            \"radlex\": [],\n",
    "            \"cxrgraph_ent\": [],\n",
    "            \"cxrgraph_attr\": [],\n",
    "            \"cxrgraph_rel\": [],\n",
    "        }\n",
    "\n",
    "        for sent_idx, sent_radlex in enumerate(ds_text_data_row[\"radlex\"]):\n",
    "            is_contain_target_radlex = False\n",
    "\n",
    "            # 判断这个句子是否包含目标radlex节点\n",
    "            for radlex_item in sent_radlex:\n",
    "                curr_radlex_node = radlex_nodes_dict[radlex_item[\"radlex_id\"]]\n",
    "                if target_radlex_node_id == curr_radlex_node.class_id or target_radlex_node_id in curr_radlex_node.all_parents:\n",
    "                    is_contain_target_radlex = True\n",
    "                    break\n",
    "\n",
    "            # 如果句子包含目标radlex节点，那么就把这个句子的相关内容，比如radlex， cxrgraph 加入到数据集中\n",
    "            # radcoref暂时不需要，因为我们只关注一个radlex节点，不需要判断句子之间的关系。在img2sent时，默认生成所有句子\n",
    "            if is_contain_target_radlex:\n",
    "                output_data_row[\"split_sents\"].append(ds_text_data_row[\"split_sents\"][sent_idx])\n",
    "                output_data_row[\"split_sent_toks\"].append(ds_text_data_row[\"split_sent_toks\"][sent_idx])\n",
    "                output_data_row[\"sent_idx_split_idx\"].append(ds_text_data_row[\"sent_idx_split_idx\"][sent_idx])\n",
    "                output_data_row[\"radlex\"].append(ds_text_data_row[\"radlex\"][sent_idx])\n",
    "                output_data_row[\"cxrgraph_ent\"].append(ds_text_data_row[\"cxrgraph_ent\"][sent_idx])\n",
    "                output_data_row[\"cxrgraph_attr\"].append(ds_text_data_row[\"cxrgraph_attr\"][sent_idx])\n",
    "                output_data_row[\"cxrgraph_rel\"].append(ds_text_data_row[\"cxrgraph_rel\"][sent_idx])\n",
    "\n",
    "        assert len(output_data_row[\"split_sents\"]) != 0\n",
    "\n",
    "        yield output_data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_impr_ds = Dataset.from_generator(ds_generator, gen_kwargs={\"target_section_name\": \"impression\", \"target_data_split\": \"test\", \"target_radlex_node_id\": target_node.class_id})\n",
    "test_impr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_text/effusion\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for section_name in [\"findings\", \"impression\"]:\n",
    "    ds_dict = {}\n",
    "    for data_split in [\"train\", \"validation\", \"test\"]:\n",
    "        ds_dict[data_split] = Dataset.from_generator(ds_generator, gen_kwargs={\"target_section_name\": section_name, \"target_data_split\": data_split, \"target_radlex_node_id\": target_node.class_id})\n",
    "    dataset_dict_final = DatasetDict(ds_dict)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"interpret_text_{section_name}_effusion\")\n",
    "    dataset_dict_final.save_to_disk(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrg_preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
