{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一个方法使用graph repr并不能让model有效的关注到图像中的重点。可能是graph太复杂了，杂乱的信息太多，因为我们把所有的ent和rel都放了进去。\n",
    "\n",
    "这里尝试提前筛选最重要的部分，避免在graph中保留太多信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get root nodes from CXRGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/anaconda3/envs/arrg_preprocessing/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import bisect\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity:\n",
    "    def __init__(self, start, end, label, sent_id, tok_list=None, tok_str=None):\n",
    "        self.id = None\n",
    "        self.tok_indices = [start, end]\n",
    "        self.label = label\n",
    "\n",
    "        self.sent_id = sent_id\n",
    "        if tok_list:\n",
    "            self.tok_list = tok_list\n",
    "            self.tok_str = \" \".join(tok_list) if not tok_str else tok_str\n",
    "        elif tok_str:\n",
    "            self.tok_str = tok_str\n",
    "            self.tok_list = tok_str.split(\" \")\n",
    "\n",
    "        if \"Observation\" in label:\n",
    "            self.label_type = \"OBS\"\n",
    "        elif \"Anatomy\" == label:\n",
    "            self.label_type = \"ANAT\"\n",
    "        else:\n",
    "            self.label_type = \"LOCATT\"\n",
    "\n",
    "        self.attr_normal = \"NA\"\n",
    "        self.attr_action = \"NA\"\n",
    "        self.attr_change = \"NA\"\n",
    "\n",
    "        self.chain_info = {\n",
    "            \"modify\": {\"from\": [], \"to\": []},\n",
    "            \"part_of\": {\"from\": [], \"to\": []},\n",
    "            \"located_at\": {\"from\": [], \"to\": []},\n",
    "            \"suggestive_of\": {\"from\": [], \"to\": []},\n",
    "        }\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # return f\"{self.tok_str} {self.tok_indices}: {self.label}, {self.attr_normal, self.attr_action, self.attr_change}\"\n",
    "        return f\"{self.tok_str}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Entity):\n",
    "            return self.tok_indices == other.tok_indices\n",
    "        else:\n",
    "            return other == self.tok_indices\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self.tok_indices))\n",
    "\n",
    "\n",
    "class Relation:\n",
    "    def __init__(self, subj_ent, obj_ent, label):\n",
    "        self.label = label\n",
    "        self.subj_ent = subj_ent\n",
    "        self.obj_ent = obj_ent\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.subj_ent.tok_str} {self.label} {self.obj_ent.tok_str}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "class LinkedGraph:\n",
    "    def __init__(self, ents):\n",
    "        self.id = None\n",
    "        self.ents = sorted(ents, key=lambda x: x.tok_indices[0])\n",
    "        self.rels = []\n",
    "        self.sent_id = ents[0].sent_id\n",
    "\n",
    "        assert len(set([i.sent_id for i in ents])) == 1\n",
    "\n",
    "    def get_involved_rels(self, rel_list):\n",
    "        target_rels = []\n",
    "        in_used_ents = set()\n",
    "        for rel in rel_list:\n",
    "            if rel.subj_ent in self.ents and rel.obj_ent in self.ents:\n",
    "                target_rels.append(rel)\n",
    "                in_used_ents.update([rel.subj_ent, rel.obj_ent])\n",
    "        self.rels = target_rels\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{[i.tok_str for i in self.ents]}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_linked_ents(curr_ent, visited, group):\n",
    "    visited.add(curr_ent)\n",
    "    group.append(curr_ent)\n",
    "    neighbors = [ent for nested_dict in curr_ent.chain_info.values() for adjacent_ents in nested_dict.values() for ent in adjacent_ents]\n",
    "    for next_ent in neighbors:\n",
    "        if next_ent not in visited:\n",
    "            search_linked_ents(next_ent, visited, group)\n",
    "\n",
    "\n",
    "def max_coverage_spans(spans):\n",
    "    if not spans:\n",
    "        return [], [], 0\n",
    "\n",
    "    # 按结束时间升序排序\n",
    "    sorted_spans = sorted(spans, key=lambda x: x[1])\n",
    "    n = len(sorted_spans)\n",
    "    starts = [s[0] for s in sorted_spans]\n",
    "    ends = [s[1] for s in sorted_spans]\n",
    "    lengths = [e - s for s, e in sorted_spans]\n",
    "\n",
    "    # 预处理j_values数组，记录每个i对应的最大的j，使得 ends[j] <= starts[i]\n",
    "    j_values = []\n",
    "    for i in range(n):\n",
    "        start_i = starts[i]\n",
    "        j = bisect.bisect_right(ends, start_i) - 1  # 二分查找, 找到第一个`大于`start_i的位置\n",
    "        j_values.append(j)\n",
    "\n",
    "    # 构建 dp 数组，其中 dp[i] 表示前 i+1 个 span 的最大总覆盖率。通过比较包含当前 span 和不包含当前 span 的情况，确定最优解。\n",
    "    # dp记录了选中下一个span之后的总覆盖率\n",
    "    dp = [0] * n\n",
    "    dp[0] = lengths[0]\n",
    "    for i in range(1, n):\n",
    "        j = j_values[i]\n",
    "        current = lengths[i] + (dp[j] if j >= 0 else 0)\n",
    "        dp[i] = max(dp[i - 1], current)\n",
    "\n",
    "    # 回溯找出选中的span。从最后一个span开始，如果当前span被选中，则跳到j_values[i]对应的span\n",
    "    # 当dp发生变化时，说明\n",
    "    selected_indices = []\n",
    "    i = n - 1\n",
    "    while i >= 0:\n",
    "        if i == 0:\n",
    "            if dp[i] == lengths[i]:\n",
    "                selected_indices.append(i)\n",
    "            break\n",
    "        if dp[i] > dp[i - 1]:\n",
    "            selected_indices.append(i)\n",
    "            i = j_values[i]\n",
    "        else:\n",
    "            i -= 1\n",
    "\n",
    "    selected_indices.reverse()\n",
    "    selected_spans = [sorted_spans[i] for i in selected_indices]\n",
    "    total_coverage = dp[-1]\n",
    "\n",
    "    return selected_indices, selected_spans, total_coverage\n",
    "\n",
    "\n",
    "def resolve_ent_rel(split_sent_idx, cxrgraph_ent_lst, cxrgraph_rel_lst, cxrgraph_attr_lst, radlex_lst):\n",
    "    # Entity and Relation 使用的是cxrgraph的结果，radlex则是用来 normalize cxrgraph的ent\n",
    "    ent_list = []\n",
    "    rel_list = []\n",
    "    for ent in cxrgraph_ent_lst:\n",
    "        ent = Entity(start=ent[\"tok_indices\"][0], end=ent[\"tok_indices\"][1], label=ent[\"ent_type\"], tok_list=ent[\"ent_toks\"], sent_id=split_sent_idx)\n",
    "        ent_list.append(ent)\n",
    "    for attr in cxrgraph_attr_lst:\n",
    "        ent = ent_list[ent_list.index(attr[\"tok_indices\"])]\n",
    "        ent.attr_normal = attr[\"attr_normality\"]\n",
    "        ent.attr_action = attr[\"attr_action\"]\n",
    "        ent.attr_change = attr[\"attr_change\"]\n",
    "    for rel in cxrgraph_rel_lst:\n",
    "        subj_ent = ent_list[ent_list.index(rel[\"subj_tok_indices\"])]\n",
    "        obj_ent = ent_list[ent_list.index(rel[\"obj_tok_indices\"])]\n",
    "        label = rel[\"rel_type\"]\n",
    "        if obj_ent not in subj_ent.chain_info[label][\"to\"]:\n",
    "            subj_ent.chain_info[label][\"to\"].append(obj_ent)\n",
    "        if subj_ent not in obj_ent.chain_info[label][\"from\"]:\n",
    "            obj_ent.chain_info[label][\"from\"].append(subj_ent)\n",
    "        rel_list.append(Relation(subj_ent, obj_ent, label))\n",
    "\n",
    "    # Set ent id\n",
    "    for ent_idx, ent in enumerate(sorted(ent_list, key=lambda x: x.tok_indices[0])):\n",
    "        ent.id = f\"E{ent_idx}\"\n",
    "\n",
    "    # 选择覆盖率最大的radlex子集\n",
    "    radlex_ent_indices = [node[\"tok_indices\"] for node in radlex_lst]\n",
    "    selected_idx_list, _, _ = max_coverage_spans(radlex_ent_indices)\n",
    "\n",
    "    # 用radlex的ent替换cxrgraph的ent\n",
    "    for radlex_idx in selected_idx_list:\n",
    "        radlex_ent = radlex_lst[radlex_idx]\n",
    "        merged_cxrgraph_ents = []\n",
    "        for cxrgraph_ent in ent_list:\n",
    "            # 如果cxrgrpah被radlex包含，那么就加入候选集等待替换；如果cxrgraph和radlex有交集，那么就跳过这个radlex\n",
    "            pos_ab = check_span_relation(cxrgraph_ent.tok_indices, radlex_ent[\"tok_indices\"])\n",
    "            if pos_ab in [\"equal\", \"inside\"]:\n",
    "                merged_cxrgraph_ents.append(cxrgraph_ent)\n",
    "            elif pos_ab == \"overlap\":\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # 如果merged_cxrgraph_ents不为空，那么就用radlex替换候选集的cxrgraph ent\n",
    "        if merged_cxrgraph_ents:\n",
    "            inherited_label = get_label_inheritance(merged_cxrgraph_ents)\n",
    "            inherited_attr_dict = get_attr_inheritance(merged_cxrgraph_ents)\n",
    "\n",
    "            new_ent = Entity(start=radlex_ent[\"tok_indices\"][0], end=radlex_ent[\"tok_indices\"][1], label=inherited_label, tok_str=radlex_ent[\"radlex_name\"], sent_id=split_sent_idx)\n",
    "            new_ent.attr_normal = inherited_attr_dict[\"normality\"]\n",
    "            new_ent.attr_action = inherited_attr_dict[\"action\"]\n",
    "            new_ent.attr_change = inherited_attr_dict[\"change\"]\n",
    "            new_ent.id = radlex_ent[\"radlex_id\"]\n",
    "\n",
    "            # inherit chain info\n",
    "            for cxrgraph_ent in merged_cxrgraph_ents:\n",
    "                for rel_type, from_to_dict in cxrgraph_ent.chain_info.items():\n",
    "                    # 把merged_cxrgraph_ents的from和to的关系都继承过来，如果是内部ents之间指向关系，那么就跳过\n",
    "                    for key, value_lst in from_to_dict.items():\n",
    "                        for value in value_lst:\n",
    "                            if value not in merged_cxrgraph_ents:\n",
    "                                if value not in new_ent.chain_info[rel_type][key]:\n",
    "                                    new_ent.chain_info[rel_type][key].append(value)\n",
    "\n",
    "            # replace from ent_list\n",
    "            ent_list.append(new_ent)\n",
    "            for cxrgraph_ent in merged_cxrgraph_ents:\n",
    "                ent_list.remove(cxrgraph_ent)\n",
    "\n",
    "            # replace from rel_list\n",
    "            # pleural_effusion 应该把 pleural 和 effusion 都替换掉。在rel中则包括：\n",
    "            #   opacifications suggestive_of effusions\n",
    "            #   bilateral modify pleural\n",
    "            #   effusions located_at pleural\n",
    "            rel_objs_tobe_removed = []\n",
    "            for rel in rel_list:\n",
    "                if rel.subj_ent in merged_cxrgraph_ents and rel.obj_ent in merged_cxrgraph_ents:\n",
    "                    # 关于 from 和 to 的关系链，在新的ent中已经继承了，所以这里不需要处理\n",
    "                    rel_objs_tobe_removed.append(rel)\n",
    "                elif rel.subj_ent in merged_cxrgraph_ents:\n",
    "                    # subj need to be replaced\n",
    "                    if rel.subj_ent in rel.obj_ent.chain_info[rel.label][\"from\"]:\n",
    "                        rel.obj_ent.chain_info[rel.label][\"from\"].remove(rel.subj_ent)\n",
    "                    if new_ent not in rel.obj_ent.chain_info[rel.label][\"from\"]:\n",
    "                        rel.obj_ent.chain_info[rel.label][\"from\"].append(new_ent)\n",
    "                    rel.subj_ent = new_ent\n",
    "                elif rel.obj_ent in merged_cxrgraph_ents:\n",
    "                    if rel.obj_ent in rel.subj_ent.chain_info[rel.label][\"to\"]:\n",
    "                        rel.subj_ent.chain_info[rel.label][\"to\"].remove(rel.obj_ent)\n",
    "                    if new_ent not in rel.subj_ent.chain_info[rel.label][\"to\"]:\n",
    "                        rel.subj_ent.chain_info[rel.label][\"to\"].append(new_ent)\n",
    "                    rel.obj_ent = new_ent\n",
    "\n",
    "            for rel in rel_objs_tobe_removed:\n",
    "                rel_list.remove(rel)\n",
    "\n",
    "    assert len(rel_list) == len(set(rel_list)), f\"{rel_list}\"\n",
    "    return ent_list, rel_list\n",
    "\n",
    "\n",
    "def get_label_inheritance(cxrgraph_ents):\n",
    "    candi_labels = [ent.label for ent in cxrgraph_ents]\n",
    "    if \"Observation-Absent\" in candi_labels:\n",
    "        return \"Observation-Absent\"\n",
    "    elif \"Observation-Uncertain\" in candi_labels:\n",
    "        return \"Observation-Uncertain\"\n",
    "    elif \"Observation-Present\" in candi_labels:\n",
    "        return \"Observation-Present\"\n",
    "    elif \"Anatomy\" in candi_labels:\n",
    "        return \"Anatomy\"\n",
    "    else:\n",
    "        return \"Location-Attribute\"\n",
    "\n",
    "\n",
    "def get_attr_inheritance(cxrgraph_ents):\n",
    "    candi_attr_normal = [ent.attr_normal for ent in cxrgraph_ents]\n",
    "    candi_attr_action = [ent.attr_action for ent in cxrgraph_ents]\n",
    "    candi_attr_change = [ent.attr_change for ent in cxrgraph_ents]\n",
    "    assert all([i[0].istitle() for i in candi_attr_change]), f\"{candi_attr_change} {candi_attr_normal} {candi_attr_action}\"\n",
    "\n",
    "    output_attr = {\"normality\": \"NA\", \"action\": \"NA\", \"change\": \"NA\"}\n",
    "    if \"Normal\" in candi_attr_normal:\n",
    "        output_attr[\"normality\"] = \"Normal\"\n",
    "    elif \"Abnormal\" in candi_attr_normal:\n",
    "        output_attr[\"normality\"] = \"Abnormal\"\n",
    "\n",
    "    if \"Essential\" in candi_attr_action:\n",
    "        output_attr[\"action\"] = \"Essential\"\n",
    "    elif \"Removable\" in candi_attr_action:\n",
    "        output_attr[\"action\"] = \"Removable\"\n",
    "\n",
    "    if \"Positive\" in candi_attr_change:\n",
    "        output_attr[\"change\"] = \"Positive\"\n",
    "    elif \"Negative\" in candi_attr_change:\n",
    "        output_attr[\"change\"] = \"Negative\"\n",
    "    elif \"Unchanged\" in candi_attr_change:\n",
    "        output_attr[\"change\"] = \"Unchanged\"\n",
    "\n",
    "    return output_attr\n",
    "\n",
    "\n",
    "def check_span_relation(ent_a_indices, ent_b_indices):\n",
    "    if ent_a_indices[1] <= ent_b_indices[0]:\n",
    "        return \"before\"\n",
    "    elif ent_a_indices[0] >= ent_b_indices[1]:\n",
    "        return \"after\"\n",
    "    elif ent_a_indices[0] == ent_b_indices[0] and ent_a_indices[1] == ent_b_indices[1]:\n",
    "        return \"equal\"\n",
    "    elif ent_a_indices[0] <= ent_b_indices[0] and ent_b_indices[1] <= ent_a_indices[1]:\n",
    "        return \"contain\"\n",
    "    elif ent_b_indices[0] <= ent_a_indices[0] and ent_a_indices[1] <= ent_b_indices[1]:\n",
    "        return \"inside\"\n",
    "    else:\n",
    "        return \"overlap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceRepresentation:\n",
    "    def __init__(self, doc_key, sent_id, sent_text):\n",
    "        self.doc_key = doc_key\n",
    "        self.sent_id = sent_id\n",
    "        self.sent_text = sent_text\n",
    "        self.ent_tuples = []  # (tok_str, label, attr_normal, attr_action, attr_change)\n",
    "        self.rel_tuples = []  # (subj_tok_str, label, obj_tok_str)\n",
    "        self.normal = []  # Observation-Present and Normal\n",
    "        self.abnormal = []  # Observation-Present and Abnormal\n",
    "        self.absent = []  # Observation-Absent\n",
    "        self.uncertain = []  # Observation-Uncertain\n",
    "\n",
    "    def set_sent_repr(self, linked_graphs):\n",
    "        for linked_graph in linked_graphs:\n",
    "            # 这部分是方法1 (7_construct_graph.ipynb)，直接从linked_graph中获取实体和关系，只对graph进行了最基础的处理。\n",
    "            # 从5_1_fsdp_peft_full_graph_text.py实验结果推测是graph太复杂，包含了太多的杂乱信息，导致模型无法有效学习。\n",
    "            for ent in linked_graph.ents:\n",
    "                if ent.attr_action == \"Removable\":\n",
    "                    continue\n",
    "                self.ent_tuples.append((ent.tok_str, ent.label, ent.attr_normal, ent.attr_action, ent.attr_change))\n",
    "\n",
    "            for rel in linked_graph.rels:\n",
    "                if rel.subj_ent.attr_action == \"Removable\" or rel.obj_ent.attr_action == \"Removable\":\n",
    "                    continue\n",
    "                self.rel_tuples.append((rel.subj_ent.tok_str, rel.label, rel.obj_ent.tok_str))\n",
    "\n",
    "            # 这部分是方法2：想办法对graph进行简化，避免过于复杂的graph导致模型无法学习。\n",
    "\n",
    "            # Removable 的 ents 将通过 chain_info 进行截断，即路径中直接或间接指向Removable的实体将被删除\n",
    "            removable_ents = []\n",
    "            for ent in linked_graph.ents:\n",
    "                if ent.attr_action == \"Removable\":\n",
    "                    removable_ents.append(ent)\n",
    "                    removable_ents.extend(collect_from_path_ents_via_chain_info(ent))\n",
    "\n",
    "            # 疾病实体分为正常、异常、缺失和不确定四类：\n",
    "            is_normal = False\n",
    "            is_absent = False\n",
    "            is_uncertain = False\n",
    "            repr_nodes = []\n",
    "            for ent in linked_graph.ents:\n",
    "                if ent in removable_ents:\n",
    "                    continue\n",
    "                if ent.label == \"Observation-Absent\":\n",
    "                    is_absent = True\n",
    "                if ent.label == \"Observation-Present\" and ent.attr_normal == \"Normal\":\n",
    "                    is_normal = True\n",
    "                if ent.label == \"Observation-Uncertain\" or ent.chain_info[\"suggestive_of\"][\"to\"]:\n",
    "                    # 如果ent的label是Observation-Uncertain，或者ents之间存在suggestive_of关系（任意一个ent有就可以，因为rel都是成对的）\n",
    "                    is_uncertain = True\n",
    "                repr_nodes.append(ent)\n",
    "\n",
    "            # 此外，额外增加一些规则来处理数据：\n",
    "            # 构建 modify_group：所有通过 modify 连接的实体合并为一个组（连通分量），组内实体按 tok_indices[0] 升序排序。\n",
    "            # 组间依赖关系：见 reorder_entities()\n",
    "            reorder_nodes = reorder_entities(repr_nodes)\n",
    "            repr_str_list = [node if isinstance(node, str) else node.tok_str.lower() for node in reorder_nodes]  # 如果isinstance(node, str)==True，则当前node是特殊的关系字符串：e.g. <suggestive_of>\n",
    "\n",
    "            if is_uncertain:\n",
    "                self.uncertain.append(\" \".join(repr_str_list))\n",
    "            elif is_absent:\n",
    "                self.absent.append(\" \".join(repr_str_list))\n",
    "            elif is_normal:\n",
    "                self.normal.append(\" \".join(repr_str_list))\n",
    "            else:\n",
    "                self.abnormal.append(\" \".join(repr_str_list))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.sent_text}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "class ReprTuple:\n",
    "    def __init__(self, repr_tuples):\n",
    "        self.repr_tuples = tuple(sorted(tuple(sorted(inner)) if isinstance(inner, list) else inner for inner in repr_tuples))\n",
    "\n",
    "    def __hash__(self):\n",
    "        # 直接对已经排序的嵌套元组进行哈希\n",
    "        return hash(self.repr_tuples)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        # 比较两个对象的 repr_tuples 是否相等\n",
    "        if isinstance(other, ReprTuple):\n",
    "            return self.repr_tuples == other.repr_tuples\n",
    "        return False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ReprTuple({self.repr_tuples})\"\n",
    "\n",
    "\n",
    "def collect_from_path_ents_via_chain_info(entity, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    # 防止重复访问\n",
    "    if entity in visited:\n",
    "        return set()\n",
    "\n",
    "    visited.add(entity)\n",
    "    result = set()\n",
    "\n",
    "    for relation_type, from_to_dict in entity.chain_info.items():\n",
    "        for ent in from_to_dict[\"from\"]:\n",
    "            if ent not in visited:\n",
    "                result.add(ent)\n",
    "                result.update(collect_from_path_ents_via_chain_info(ent, visited))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这部分代码由ChatGpt迭代生成\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "suggestive_of_label = \"<suggestive_of>\"\n",
    "localed_at_label = \"<located_at>\"\n",
    "\n",
    "\n",
    "def reorder_entities(ent_list):\n",
    "    \"\"\"\n",
    "    构建 modify + part_of group 并排序实体：\n",
    "    - group 内部先按 part_of 拓扑排序（若无 part_of 边，则直接按 tok_indices[0] 排序）；\n",
    "      若检测到环，会自动断开一条 part_of 边后重试；若仍无解，则退回按 tok_indices 排序。\n",
    "    - 对每个 group 调用 prune_redundant_head_entities() 删除冗余的单词实体。\n",
    "    - group 间依赖关系：\n",
    "        A part_of B ⇒ A 在 B 后\n",
    "        A located_at B ⇒ A 在 B 前 ⇒ 插入标签\n",
    "        A suggestive_of B ⇒ A 在 B 前 ⇒ 插入标签\n",
    "      若两种关系都存在，则依次插入 \"SUGG\"、\"LOC\"。\n",
    "    - group 间拓扑排序中的循环检测与打断，遵循组件原始顺序；\n",
    "    \"\"\"\n",
    "    # Step 1: 构建 modify + part_of 连通分量（核心 group）\n",
    "    visited = set()\n",
    "    components = []\n",
    "\n",
    "    def dfs(ent, group):\n",
    "        visited.add(ent)\n",
    "        group.append(ent)\n",
    "        neighbors = ent.chain_info[\"modify\"][\"to\"] + ent.chain_info[\"modify\"][\"from\"] + ent.chain_info[\"part_of\"][\"to\"] + ent.chain_info[\"part_of\"][\"from\"]\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor in ent_list and neighbor not in visited:\n",
    "                dfs(neighbor, group)\n",
    "\n",
    "    for ent in ent_list:\n",
    "        if ent not in visited:\n",
    "            group = []\n",
    "            dfs(ent, group)\n",
    "            group = sort_group_by_part_of(group)\n",
    "            group = prune_redundant_head_entities(group)\n",
    "            components.append(group)\n",
    "\n",
    "    # Step 2: 构建 group 级别的依赖图\n",
    "    ent_to_group = {ent: i for i, group in enumerate(components) for ent in group}\n",
    "    graph = defaultdict(set)\n",
    "    in_degree = defaultdict(int)  # 记录每个 group 的入度\n",
    "    edge_types = defaultdict(set)  # (from_group, to_group): {\"suggestive_of\", \"located_at\", ...}\n",
    "\n",
    "    def add_edge(from_ent, to_ent, rel_type, reverse=False):\n",
    "        if from_ent in ent_to_group and to_ent in ent_to_group:\n",
    "            u = ent_to_group[from_ent]\n",
    "            v = ent_to_group[to_ent]\n",
    "            if reverse:\n",
    "                u, v = v, u\n",
    "            if u != v and v not in graph[u]:\n",
    "                graph[u].add(v)\n",
    "                in_degree[v] += 1\n",
    "                edge_types[(u, v)].add(rel_type)\n",
    "\n",
    "    for group in components:\n",
    "        for ent in group:\n",
    "            for to_ent in ent.chain_info[\"part_of\"][\"to\"]:\n",
    "                # A part_of B， B → A\n",
    "                add_edge(to_ent, ent, \"part_of\")\n",
    "            for to_ent in ent.chain_info[\"located_at\"][\"to\"]:\n",
    "                # A located_at B， A → B\n",
    "                add_edge(ent, to_ent, \"located_at\")\n",
    "            for to_ent in ent.chain_info[\"suggestive_of\"][\"to\"]:\n",
    "                # A suggestive_of B， A → B\n",
    "                add_edge(ent, to_ent, \"suggestive_of\")\n",
    "\n",
    "    # Step 3: 对 group 进行拓扑排序并拼接结果\n",
    "    # 支持 group 间拓扑排序中的循环检测与打断，遵循组件原始顺序；\n",
    "    def attempt_topo_sort(edges_to_ignore=None):\n",
    "        in_deg_copy = defaultdict(int)\n",
    "        graph_copy = defaultdict(set)\n",
    "        for u in graph:\n",
    "            for v in graph[u]:\n",
    "                if edges_to_ignore and (u, v) in edges_to_ignore:\n",
    "                    continue\n",
    "                graph_copy[u].add(v)\n",
    "                in_deg_copy[v] += 1\n",
    "\n",
    "        for i in range(len(components)):\n",
    "            in_deg_copy[i] = in_deg_copy.get(i, 0)\n",
    "\n",
    "        queue = deque(i for i in range(len(components)) if in_deg_copy[i] == 0)\n",
    "        sorted_ids = []\n",
    "        while queue:\n",
    "            i = queue.popleft()\n",
    "            sorted_ids.append(i)\n",
    "            for j in graph_copy[i]:\n",
    "                in_deg_copy[j] -= 1\n",
    "                if in_deg_copy[j] == 0:\n",
    "                    queue.append(j)\n",
    "        return sorted_ids\n",
    "\n",
    "    sorted_group_ids = attempt_topo_sort()\n",
    "    if len(sorted_group_ids) != len(components):\n",
    "        all_edges = {(u, v) for u in graph for v in graph[u]}\n",
    "        group_order = {i: idx for idx, i in enumerate(range(len(components)))}\n",
    "        candidate_edges = [edge for edge in all_edges if group_order[edge[0]] > group_order[edge[1]]]\n",
    "\n",
    "        for edge in candidate_edges:\n",
    "            trial_ids = attempt_topo_sort(edges_to_ignore={edge})\n",
    "            if len(trial_ids) == len(components):\n",
    "                print(f\"Removed backward edge to break cycle: group{edge[0]} → group{edge[1]}\")\n",
    "                sorted_group_ids = trial_ids\n",
    "                break\n",
    "        else:\n",
    "            print(\"Unresolvable inter-group cycle — fallback to original group order\")\n",
    "            sorted_group_ids = list(range(len(components)))\n",
    "\n",
    "    result = []\n",
    "    for idx, gid in enumerate(sorted_group_ids):\n",
    "        result.extend(components[gid])\n",
    "        if idx < len(sorted_group_ids) - 1:\n",
    "            next_gid = sorted_group_ids[idx + 1]\n",
    "            key = (gid, next_gid)\n",
    "            if key in edge_types:\n",
    "                if \"suggestive_of\" in edge_types[key]:\n",
    "                    result.append(suggestive_of_label)\n",
    "                if \"located_at\" in edge_types[key]:\n",
    "                    result.append(localed_at_label)\n",
    "    return result\n",
    "\n",
    "\n",
    "def sort_group_by_part_of(group):\n",
    "    \"\"\"\n",
    "    对单个 group 内部进行排序：\n",
    "    1. 构建基于 part_of 的有向图（edge: B → A, 表示 A part_of B）\n",
    "    2. 尝试 Kahn 拓扑排序，每次从“零入度集合”中选择 tok_indices[0] 最小的实体；\n",
    "       若能排完全部节点，直接返回 topo 顺序。\n",
    "    3. 若出现 cycle，尝试逐条移除 part_of 边并重试（第一条能使图无环即停止），\n",
    "       并打印被移除的边信息；若所有单边移除仍无法排序，则 fallback 回\n",
    "       tok_indices[0] 排序。\n",
    "    \"\"\"\n",
    "    ent_to_index = {ent: i for i, ent in enumerate(group)}\n",
    "\n",
    "    def build_graph(edges_to_ignore=None):\n",
    "        graph = defaultdict(set)\n",
    "        in_deg = defaultdict(int)\n",
    "        for ent in group:\n",
    "            for to_ent in ent.chain_info[\"part_of\"][\"to\"]:\n",
    "                if to_ent in ent_to_index:\n",
    "                    # 建边：B → A\n",
    "                    if edges_to_ignore and (ent, to_ent) in edges_to_ignore:\n",
    "                        continue\n",
    "                    graph[to_ent].add(ent)\n",
    "                    in_deg[ent] += 1\n",
    "        return graph, in_deg\n",
    "\n",
    "    # 尝试一次正常拓扑排序\n",
    "    def attempt_topo(edges_to_ignore=None):\n",
    "        graph, in_deg = build_graph(edges_to_ignore)\n",
    "        zero = [ent for ent in group if in_deg[ent] == 0]\n",
    "        zero.sort(key=lambda x: x.tok_indices[0])\n",
    "\n",
    "        sorted_group = []\n",
    "        in_deg = in_deg.copy()\n",
    "\n",
    "        while zero:\n",
    "            # 每次取 tok_indices[0] 最小的实体\n",
    "            ent = zero.pop(0)\n",
    "            sorted_group.append(ent)\n",
    "            for nbr in list(graph[ent]):\n",
    "                in_deg[nbr] -= 1\n",
    "                if in_deg[nbr] == 0:\n",
    "                    zero.append(nbr)\n",
    "            zero.sort(key=lambda x: x.tok_indices[0])\n",
    "\n",
    "        return sorted_group\n",
    "\n",
    "    # 第一次尝试\n",
    "    sorted_group = attempt_topo(edges_to_ignore=None)\n",
    "    if len(sorted_group) == len(group):\n",
    "        return sorted_group\n",
    "\n",
    "    # 检测到环，收集所有 part_of 边\n",
    "    all_edges = set()\n",
    "    for ent in group:\n",
    "        for to_ent in ent.chain_info[\"part_of\"][\"to\"]:\n",
    "            if to_ent in ent_to_index:\n",
    "                all_edges.add((ent, to_ent))\n",
    "\n",
    "    # 逐条移除边并重试\n",
    "    for removed_edge in all_edges:\n",
    "        trial = attempt_topo(edges_to_ignore={removed_edge})\n",
    "        if len(trial) == len(group):\n",
    "            # 成功打破环\n",
    "            print(f\"Removed part_of edge to break cycle: \" f\"{removed_edge[0].tok_str} part_of {removed_edge[1].tok_str}\")\n",
    "            return trial\n",
    "\n",
    "    # 仍无法打破环，fallback：按 tok_indices 升序排序\n",
    "    print(\"Unresolvable cycle in part_of within group—fallback to tok_indices order\")\n",
    "    return sorted(group, key=lambda x: x.tok_indices[0])\n",
    "\n",
    "\n",
    "def prune_redundant_head_entities(group):\n",
    "    \"\"\"由于graph是pred的，存在错误，因此使用规则来移除明显的错误\n",
    "    在单个 group 内部，如果 ent 满足：\n",
    "      1. tok_str 是一个单词（无空格）; 只有一个 “to/from” 连接; 该 tok_str 出现在 group 中后续任意一个实体的 tok_str 里\n",
    "      4. 该 tok_str 是一个 stop word\n",
    "    则将其从 group 中移除。\n",
    "    \"\"\"\n",
    "    to_remove = set()\n",
    "    for i, ent in enumerate(group):\n",
    "        if ent.tok_str in nlp.Defaults.stop_words:\n",
    "            to_remove.add(ent)\n",
    "            continue\n",
    "\n",
    "        if \" \" in ent.tok_str:\n",
    "            continue\n",
    "\n",
    "        # 统计所有 to/from 连接\n",
    "        all_links = []\n",
    "        for rel in [\"modify\", \"part_of\", \"located_at\", \"suggestive_of\"]:\n",
    "            all_links.extend(ent.chain_info[rel][\"to\"])\n",
    "            all_links.extend(ent.chain_info[rel][\"from\"])\n",
    "        if len(all_links) != 1:\n",
    "            continue\n",
    "\n",
    "        # 如果 ent.tok_str 出现在后续任何实体的 tok_str 中，就删\n",
    "        if any(ent.tok_str in other.tok_str for other in group[i + 1 :]):\n",
    "            to_remove.add(ent)\n",
    "\n",
    "    for ent in to_remove:\n",
    "        group.remove(ent)\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_graph_repr(doc):\n",
    "    reprs = []\n",
    "    for split_sent_idx, (cxrgraph_ent, cxrgraph_rel, cxrgraph_attr, radlex) in enumerate(zip(doc[\"cxrgraph_ent\"], doc[\"cxrgraph_rel\"], doc[\"cxrgraph_attr\"], doc[\"radlex\"])):\n",
    "\n",
    "        sent_repr = SentenceRepresentation(doc_key=doc[\"doc_key\"], sent_id=split_sent_idx, sent_text=doc[\"split_sents\"][split_sent_idx])\n",
    "\n",
    "        # resolve ent and rel from json\n",
    "        ent_list, rel_list = resolve_ent_rel(split_sent_idx, cxrgraph_ent, cxrgraph_rel, cxrgraph_attr, radlex)\n",
    "\n",
    "        linked_graphs = []\n",
    "        visited_ents = set()\n",
    "        for ent in ent_list:\n",
    "            if ent not in visited_ents:\n",
    "                sent_ents = []\n",
    "                search_linked_ents(ent, visited_ents, sent_ents)\n",
    "                sent_graph = LinkedGraph(sent_ents)\n",
    "                sent_graph.get_involved_rels(rel_list)\n",
    "                linked_graphs.append(sent_graph)\n",
    "\n",
    "        sent_repr.set_sent_repr(linked_graphs)\n",
    "        reprs.append(sent_repr)\n",
    "\n",
    "    doc[\"graph_reprs2\"] = []\n",
    "    for sent_repr in reprs:\n",
    "        doc[\"graph_reprs2\"].append(\n",
    "            {\n",
    "                \"normal\": sent_repr.normal,\n",
    "                \"abnormal\": sent_repr.abnormal,\n",
    "                \"absent\": sent_repr.absent,\n",
    "                \"uncertain\": sent_repr.uncertain,\n",
    "            }\n",
    "        )\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (6/6 shards): 100%|██████████| 343738/343738 [00:01<00:00, 224709.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8825/8825 [00:00<00:00, 224193.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2692/2692 [00:00<00:00, 133331.76 examples/s]\n",
      "Saving the dataset (6/6 shards): 100%|██████████| 365565/365565 [00:02<00:00, 137905.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 9308/9308 [00:00<00:00, 70308.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2965/2965 [00:00<00:00, 179673.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_text/all/\"\n",
    "output_dir = \"/home/yuxiang/liao/workspace/arrg_preprocessing/outputs/interpret_graph/4_labels/\"\n",
    "\n",
    "for section_name in [\"findings\", \"impression\"]:\n",
    "    ds_text = load_from_disk(os.path.join(input_dir, f\"interpret_text_{section_name}\"))\n",
    "    ds_graph = ds_text.map(add_graph_repr)\n",
    "    ds_graph.save_to_disk(os.path.join(output_dir, f\"interpret_graph_{section_name}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrg_preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
